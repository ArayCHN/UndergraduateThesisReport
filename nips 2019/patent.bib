
@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{schulman2015high,
	title={High-dimensional continuous control using generalized advantage estimation},
	author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
	journal={arXiv preprint arXiv:1506.02438},
	year={2015}
}

@article{heess2016learning,
	title={Learning and transfer of modulated locomotor controllers},
	author={Heess, Nicolas and Wayne, Greg and Tassa, Yuval and Lillicrap, Timothy and Riedmiller, Martin and Silver, David},
	journal={arXiv preprint arXiv:1610.05182},
	year={2016}
}

@article{gupta2017learning,
	title={Learning invariant feature spaces to transfer skills with reinforcement learning},
	author={Gupta, Abhishek and Devin, Coline and Liu, YuXuan and Abbeel, Pieter and Levine, Sergey},
	journal={arXiv preprint arXiv:1703.02949},
	year={2017}
}

@article{sutton1999between,
	title={Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning},
	author={Sutton, Richard S and Precup, Doina and Singh, Satinder},
	journal={Artificial intelligence},
	volume={112},
	number={1-2},
	pages={181--211},
	year={1999},
	publisher={Elsevier}
}

@article{florensa2017stochastic,
	title={Stochastic neural networks for hierarchical reinforcement learning},
	author={Florensa, Carlos and Duan, Yan and Abbeel, Pieter},
	journal={arXiv preprint arXiv:1704.03012},
	year={2017}
}

@inproceedings{duan2016benchmarking,
	title={Benchmarking deep reinforcement learning for continuous control},
	author={Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
	booktitle={International Conference on Machine Learning},
	pages={1329--1338},
	year={2016}
}

@article{Eysenbach2017,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1802.06070v6},
	author = {Eysenbach, Benjamin and Berkeley, U C and Brain, Google},
	eprint = {arXiv:1802.06070v6},
	file = {:Users/lsy/Documents/papers/exploration/diversity is all you need.pdf:pdf},
	pages = {1--22},
	title = {{Diversity is all you need}},
	year = {2017}
}

@article{Haarnoja2017,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1702.08165v2},
	author = {Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
	eprint = {arXiv:1702.08165v2},
	file = {:Users/lsy/Documents/papers/DRL algs/Reinforcement Learning with Deep Energy-Based Policies.pdf:pdf},
	title = {{Reinforcement Learning with Deep Energy-Based Policies}},
	year = {2017}
}

@article{florensa2017stochastic,
	title={Stochastic Neural Networks for Hierarchical Reinforcement Learning},
	author={Florensa, Carlos and Duan, Yan and Abbeel, Pieter},
	journal={international conference on learning representations},
	year={2017}}


@article{silver2013lifelong,
	title={Lifelong Machine Learning Systems: Beyond Learning Algorithms},
	author={Silver, Daniel L and Yang, Qiang and Li, Lianghao},
	pages={49},
	year={2013}}

@article{schwarz2018progress,
	title={Progress and Compress: A scalable framework for continual learning},
	author={Schwarz, Jonathan and Czarnecki, Wojciech Marian and Luketina, Jelena and Grabskabarwinska, Agnieszka and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},
	journal={international conference on machine learning},
	pages={4528--4537},
	year={2018}}

@article{shu2018hierarchical,
	title={Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning},
	author={Shu, Tianmin and Xiong, Caiming and Socher, Richard},
	journal={international conference on learning representations},
	year={2018}}

@article{tessler2016a,
	title={A Deep Hierarchical Approach to Lifelong Learning in Minecraft.},
	author={Tessler, Chen and Givony, Shahar and Zahavy, Tom and Mankowitz, Daniel J and Mannor, Shie},
	journal={national conference on artificial intelligence},
	pages={1553--1561},
	year={2016}}

@book{sutton1998reinforcement,
	title={Reinforcement learning: An introduction},
	author={Sutton, Richard S and Barto, Andrew G},
	volume={1},
	number={1},
	year={1998},
	publisher={MIT press Cambridge}
}

@article{Ring1994,
	author = {Ring, Mark},
	file = {:Users/lsy/Downloads/download.pdf:pdf},
	number = {August 1994},
	title = {{CONTINUAL LEARNING IN REINFORCEMENT by The University of Texas at Austin in Partial Ful llment of the Requirements}},
	year = {1994}
}

@article{brunskill2014pac-inspired,
	title={PAC-inspired Option Discovery in Lifelong Reinforcement Learning},
	author={Brunskill, Emma and Li, Lihong},
	pages={316--324},
	year={2014}}

@article{tessler2016a,
	title={A Deep Hierarchical Approach to Lifelong Learning in Minecraft.},
	author={Tessler, Chen and Givony, Shahar and Zahavy, Tom and Mankowitz, Daniel J and Mannor, Shie},
	journal={national conference on artificial intelligence},
	pages={1553--1561},
	year={2016}}

@article{ammar2015autonomous,
	title={Autonomous cross-domain knowledge transfer in lifelong policy gradient reinforcement learning},
	author={Ammar, Haitham Bou and Eaton, Eric and Luna, Jose Marcio and Ruvolo, Paul},
	pages={3345--3351},
	year={2015}}

@article{fernandez2013learning,
	title={Learning domain structure through probabilistic policy reuse in reinforcement learning},
	author={Fernandez, Fernando and Veloso, Manuela M},
	journal={Progress in Artificial Intelligence},
	volume={2},
	number={1},
	pages={13--27},
	year={2013}}

@article{li2017optimal,
	title={An Optimal Online Method of Selecting Source Policies for Reinforcement Learning},
	author={Li, Siyuan and Zhang, Chongjie},
	journal={arXiv preprint arXiv:1709.08201},
	year={2017}
}

@article{li2018context,
	title={Context-Aware Policy Reuse},
	author={Li, Siyuan and Gu, Fangda and Zhu, Guangxiang and Zhang, Chongjie},
	journal={arXiv preprint arXiv:1806.03793},
	year={2018}
}

@inproceedings{todorov2012mujoco,
	title={Mujoco: A physics engine for model-based control},
	author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
	booktitle={Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on},
	pages={5026--5033},
	year={2012},
	organization={IEEE}
}

@inproceedings{kulkarni2016hierarchical,
	title={Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation},
	author={Kulkarni, Tejas D and Narasimhan, Karthik and Saeedi, Ardavan and Tenenbaum, Josh},
	booktitle={Advances in neural information processing systems},
	pages={3675--3683},
	year={2016}
}

@inproceedings{nachum2018data,
	title={Data-efficient hierarchical reinforcement learning},
	author={Nachum, Ofir and Gu, Shixiang Shane and Lee, Honglak and Levine, Sergey},
	booktitle={Advances in Neural Information Processing Systems},
	pages={3303--3313},
	year={2018}
}

@article{levy2017hierarchical,
	title={Hierarchical actor-critic},
	author={Levy, Andrew and Platt, Robert and Saenko, Kate},
	journal={arXiv preprint arXiv:1712.00948},
	year={2017}
}

@inproceedings{andrychowicz2017hindsight,
	title={Hindsight experience replay},
	author={Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, OpenAI Pieter and Zaremba, Wojciech},
	booktitle={Advances in Neural Information Processing Systems},
	pages={5048--5058},
	year={2017}
}

@article{berseth2018progressive,
	title={Progressive reinforcement learning with distillation for multi-skilled motion control},
	author={Berseth, Glen and Xie, Cheng and Cernek, Paul and Van de Panne, Michiel},
	journal={arXiv preprint arXiv:1802.04765},
	year={2018}
}






