
@book{Sutton_book,
  author = {Sutton, Richard S. and Barto, Andrew G.},
  edition = {Second},
  publisher = {The MIT Press},
  title = {Reinforcement Learning: An Introduction},
  % url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018}
}

@Article{Sutton_problem_formulation,
author="Sutton, Richard S.",
title="Learning to predict by the methods of temporal differences",
journal="Machine Learning",
year="1988",
month="Aug",
day="01",
volume="3",
number="1",
pages="9--44",
issn="1573-0565",
% doi="10.1007/BF00115009",
% url="https://doi.org/10.1007/BF00115009"
}

@ARTICLE{Bellman_MDP,
    author = "Richard Bellman",
     title = "A Markovian Decision Process",
   journal = "Indiana Univ. Math. J.",
  fjournal = "Indiana University Mathematics Journal",
    volume = 6,
      year = 1957,
     issue = 4,
     pages = "679--684",
      issn = "0022-2518",
   %   coden = "IUMJAB",
   % mrclass = "",
}

@book{Bellman_DP,
 author = {Bellman, Richard Ernest},
 title = {Dynamic Programming},
 year = {2003},
 isbn = {0486428095},
 publisher = {Dover Publications, Inc.},
 % address = {New York, NY, USA},
}

@ARTICLE{n_step_bootstrapping,
       author = {{van Seijen}, Harm},
        title = "{Effective Multi-step Temporal-Difference Learning for Non-Linear Function Approximation}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Artificial Intelligence},
         year = 2016,
        month = Aug,
          eid = {arXiv:1608.05151},
        pages = {arXiv:1608.05151},
archivePrefix = {arXiv},
       eprint = {1608.05151},
 primaryClass = {cs.AI},
      %  adsurl = {https://ui.adsabs.harvard.edu/\#abs/2016arXiv160805151V},
      % adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article {DRL_for_driving,
title = "Deep Reinforcement Learning framework for Autonomous Driving",
journal = "Electronic Imaging",
% parent_itemid = "infobike://ist/ei",
% publishercode ="ist",
year = "2017",
volume = "2017",
number = "19",
publication date ="2017-01-29T00:00:00",
pages = "70-76",
itemtype = "ARTICLE",
% issn = "2470-1173",
% eissn = "2470-1173",
% url = "https://www.ingentaconnect.com/content/ist/ei/2017/00002017/00000019/art00012",
% doi = "doi:10.2352/ISSN.2470-1173.2017.19.AVM-023",
% keyword = "REINFORCEMENT LEARNING, DEEP LEARNING, AUTONOMOUS DRIVING",
author = "Sallab, Ahmad EL and Abdou, Mohammed and Perot, Etienne and Yogamani, Senthil",
}

@ARTICLE{deepRL_overview,
       author = {{Li}, Yuxi},
        title = "{Deep Reinforcement Learning: An Overview}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning},
         year = 2017,
        month = Jan,
          eid = {arXiv:1701.07274},
        pages = {arXiv:1701.07274},
archivePrefix = {arXiv},
       eprint = {1701.07274},
 primaryClass = {cs.LG},
      %  adsurl = {https://ui.adsabs.harvard.edu/\#abs/2017arXiv170107274L},
      % adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@incollection{atari_2013,
  title = {Playing Atari With Deep Reinforcement Learning},
  author = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
  booktitle = {NIPS Deep Learning Workshop},
  year = {2013}
}

@misc{openAI_baselines,
  author = {Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai and Zhokhov, Peter},
  title = {OpenAI Baselines},
  year = {2017},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openai/baselines}},
}

@inproceedings{TRPO,
  title={Trust Region Policy Optimization},
  author={John Schulman and Sergey Levine and Philipp Moritz and Michael I. Jordan and Pieter Abbeel},
  booktitle={ICML},
  year={2015}
}

@ARTICLE{DDPG,
       author = {{Lillicrap}, Timothy P. and {Hunt}, Jonathan J. and {Pritzel}, Alexander
        and {Heess}, Nicolas and {Erez}, Tom and {Tassa}, Yuval and
        {Silver}, David and {Wierstra}, Daan},
        title = "{Continuous control with deep reinforcement learning}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
         year = 2015,
        month = Sep,
          eid = {arXiv:1509.02971},
        pages = {arXiv:1509.02971},
archivePrefix = {arXiv},
       eprint = {1509.02971},
 primaryClass = {cs.LG},
      %  adsurl = {https://ui.adsabs.harvard.edu/\#abs/2015arXiv150902971L},
      % adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{ACKTR,
       author = {{Wu}, Yuhuai and {Mansimov}, Elman and {Liao}, Shun and {Grosse}, Roger
        and {Ba}, Jimmy},
        title = "{Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning},
         year = 2017,
        month = Aug,
          eid = {arXiv:1708.05144},
        pages = {arXiv:1708.05144},
archivePrefix = {arXiv},
       eprint = {1708.05144},
 primaryClass = {cs.LG},
      %  adsurl = {https://ui.adsabs.harvard.edu/\#abs/2017arXiv170805144W},
      % adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@misc{PPO,
    title={Proximal Policy Optimization Algorithms},
    author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
    year={2017},
    eprint={1707.06347},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{ACER,
  title={Sample Efficient Actor-Critic with Experience Replay},
  author={Ziyu Wang and Victor Bapst and Nicolas Heess and Volodymyr Mnih and Remi Munos and Koray Kavukcuoglu and Nando de Freitas},
  booktitle={ICLR},
  year={2017}
}

@Article{Q_learning,
author="Watkins, Christopher J. C. H.
and Dayan, Peter",
title="Q-learning",
journal="Machine Learning",
year="1992",
month="May",
day="01",
volume="8",
number="3",
pages="279--292",
% issn="1573-0565",
% doi="10.1007/BF00992698",
% url="https://doi.org/10.1007/BF00992698"
}

@Article{nature2015,
author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
title={Human-level control through deep reinforcement learning},
journal={Nature},
year={2015},
month={Feb},
day={25},
publisher={Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved. SN  -},
volume={518},
pages={529 EP  -},
% url={https://doi.org/10.1038/nature14236}
}

@article{AlexNet,
author = {Krizhevsky, Alex and Sutskever, Ilya and E. Hinton, Geoffrey},
year = {2012},
month = {01},
pages = {},
title = {ImageNet Classification with Deep Convolutional Neural Networks},
volume = {25},
journal = {Neural Information Processing Systems},
% doi = {10.1145/3065386}
}

@InProceedings{A3C,
  title =    {Asynchronous Methods for Deep Reinforcement Learning},
  author =   {Volodymyr Mnih and Adria Puigdomenech Badia and Mehdi Mirza and Alex Graves and Timothy Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
  booktitle =    {Proceedings of The 33rd International Conference on Machine Learning},
  pages =    {1928--1937},
  year =   {2016},
  editor =   {Maria Florina Balcan and Kilian Q. Weinberger},
  volume =   {48},
  series =   {Proceedings of Machine Learning Research},
  address =    {New York, New York, USA},
  month =    {20--22 Jun},
  publisher =    {PMLR},
  % pdf =    {http://proceedings.mlr.press/v48/mniha16.pdf},
  % url =    {http://proceedings.mlr.press/v48/mniha16.html},
}

@article{RMSProp,
    title={Lecture 6.5 rmsprop: Divide the gradient by a running average of its recent magnitude.},
    author={Tieleman, Tijmen and Hinton, Geoffrey},
    year={2012},
    volume = {4},
    journal = {COURSERA: Neural Networks for Machine Learning},
}

@ARTICLE{GD_overview,
       author = {{Ruder}, Sebastian},
        title = "{An overview of gradient descent optimization algorithms}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning},
         year = 2016,
        month = Sep,
          eid = {arXiv:1609.04747},
        pages = {arXiv:1609.04747},
archivePrefix = {arXiv},
       eprint = {1609.04747},
 primaryClass = {cs.LG},
      %  adsurl = {https://ui.adsabs.harvard.edu/\#abs/2016arXiv160904747R},
      % adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{SGD_momentum,
 author = {Qian, Ning},
 title = {On the Momentum Term in Gradient Descent Learning Algorithms},
 journal = {Neural Netw.},
 issue_date = {Jan. 1999},
 volume = {12},
 number = {1},
 month = jan,
 year = {1999},
 issn = {0893-6080},
 pages = {145--151},
 numpages = {7},
 % url = {http://dx.doi.org/10.1016/S0893-6080(98)00116-6},
 % doi = {10.1016/S0893-6080(98)00116-6},
 acmid = {307376},
 publisher = {Elsevier Science Ltd.},
 % address = {Oxford, UK, UK},
} 

@inproceedings{DPG,
 author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
 title = {Deterministic Policy Gradient Algorithms},
 booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
 series = {ICML'14},
 year = {2014},
 % location = {Beijing, China},
 pages = {I-387--I-395},
 % url = {http://dl.acm.org/citation.cfm?id=3044805.3044850},
 % acmid = {3044850},
 publisher = {JMLR.org},
}

@inproceedings{off-policy_actor-critic,
  title={Linear Off-Policy Actor-Critic},
  author={Thomas Degris and Martha White and Richard S. Sutton},
  booktitle={ICML},
  year={2012}
}

@article{RL_in_robotics_survey,
 author = {Kober, Jens and Bagnell, J. Andrew and Peters, Jan},
 title = {Reinforcement Learning in Robotics: A Survey},
 journal = {Int. J. Rob. Res.},
 issue_date = {September 2013},
 volume = {32},
 number = {11},
 month = sep,
 year = {2013},
 issn = {0278-3649},
 pages = {1238--1274},
 numpages = {37},
 % url = {http://dx.doi.org/10.1177/0278364913495721},
 % doi = {10.1177/0278364913495721},
 acmid = {2528334},
 publisher = {Sage Publications, Inc.},
 % address = {Thousand Oaks, CA, USA},
 % keywords = {Reinforcement learning, learning control, robot, survey},
} 

@article{AlphaGo,
  added-at = {2016-03-11T14:36:05.000+0100},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  doi = {10.1038/nature16961},
  journal = {Nature},
  month = jan,
  number = 7587,
  pages = {484--489},
  publisher = {Nature Publishing Group},
  title = {Mastering the Game of {Go} with Deep Neural Networks and Tree Search},
  volume = 529,
  year = 2016
}

@article {AlphaZero,
    author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
    title = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
    volume = {362},
    number = {6419},
    pages = {1140--1144},
    year = {2018},
    doi = {10.1126/science.aar6404},
    publisher = {American Association for the Advancement of Science},
    % issn = {0036-8075},
    % URL = {http://science.sciencemag.org/content/362/6419/1140},
    % eprint = {http://science.sciencemag.org/content/362/6419/1140.full.pdf},
    journal = {Science}
}

@book{Convex_optimization_book,
  author = {Boyd, Stephen and Vandenberghe, Lieven},
  howpublished = {Hardcover},
  isbn = {0521833787},
  month = {March},
  priority = {4},
  publisher = {{Cambridge University Press}},
  timestamp = {2006-04-12T21:13:14.000+0200},
  title = {Convex Optimization},
  year = 2004
}

@incollection{I2A,
title = {Imagination-Augmented Agents for Deep Reinforcement Learning},
author = {Racani\`{e}re, S\'{e}bastien and Weber, Theophane and Reichert, David and Buesing, Lars and Guez, Arthur and Jimenez Rezende, Danilo and Puigdom\`{e}nech Badia, Adri\`{a} and Vinyals, Oriol and Heess, Nicolas and Li, Yujia and Pascanu, Razvan and Battaglia, Peter and Hassabis, Demis and Silver, David and Wierstra, Daan},
booktitle = {Advances in Neural Information Processing Systems 30},
% editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5690--5701},
year = {2017},
publisher = {Curran Associates, Inc.},
% url = {http://papers.nips.cc/paper/7152-imagination-augmented-agents-for-deep-reinforcement-learning.pdf}
}

@incollection{action-conditional_prediction,
title = {Action-Conditional Video Prediction using Deep Networks in Atari Games},
author = {Oh, Junhyuk and Guo, Xiaoxiao and Lee, Honglak and Lewis, Richard L and Singh, Satinder},
booktitle = {Advances in Neural Information Processing Systems 28},
% editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {2863--2871},
year = {2015},
publisher = {Curran Associates, Inc.},
% url = {http://papers.nips.cc/paper/5859-action-conditional-video-prediction-using-deep-networks-in-atari-games.pdf}
}

@Article{Model-Based_RL_robotics,
author="Polydoros, Athanasios S.
and Nalpantidis, Lazaros",
title="Survey of Model-Based Reinforcement Learning: Applications on Robotics",
journal="Journal of Intelligent {\&} Robotic Systems",
year="2017",
month="May",
day="01",
volume="86",
number="2",
pages="153--173",
issn="1573-0409",
% doi="10.1007/s10846-017-0468-y",
% url="https://doi.org/10.1007/s10846-017-0468-y"
}

@article{RL_in_robotics,
 author = {Kober, Jens and Bagnell, J. Andrew and Peters, Jan},
 title = {Reinforcement Learning in Robotics: A Survey},
 journal = {Int. J. Rob. Res.},
 issue_date = {September 2013},
 volume = {32},
 number = {11},
 month = sep,
 year = {2013},
 issn = {0278-3649},
 pages = {1238--1274},
 numpages = {37},
} 


@misc{waymo,
  title = {Waymo},
  url="https://waymo.com"
}

 @inproceedings{SUMO,
          title = {Microscopic Traffic Simulation using SUMO},
         author = {Pablo Alvarez Lopez and Michael Behrisch and Laura Bieker-Walz and Jakob Erdmann and Yun-Pang Fl{\"o}tter{\"o}d and Robert Hilbrich and Leonhard L{\"u}cken and Johannes Rummel and Peter Wagner and Evamarie Wie{\ss}ner},
      publisher = {IEEE},
      booktitle = {The 21st IEEE International Conference on Intelligent Transportation Systems},
           year = {2018},
        journal = {IEEE Intelligent Transportation Systems Conference (ITSC)},
 }

 @inproceedings{Webots,
 author = {Michel, Olivier},
 title = {Webots: Symbiosis Between Virtual and Real Mobile Robots},
 booktitle = {Proceedings of the First International Conference on Virtual Worlds},
 series = {VW '98},
 year = {1998},
 isbn = {3-540-64780-5},
 pages = {254--263},
 numpages = {10}
} 

@inproceedings{DRQ,
  title={Deep Recurrent Q-Learning for Partially Observable MDPs},
  author={Matthew J. Hausknecht and Peter Stone},
  booktitle={AAAI Fall Symposia},
  year={2015}
}

@article{LSTM,
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal = {Neural computation},
  number = 8,
  pages = {1735--1780},
  publisher = {MIT Press},
  title = {Long short-term memory},
  volume = 9,
  year = 1997
}

@misc{game_theory,
    title={Adaptive Game-Theoretic Decision Making for Autonomous Vehicle Control at Roundabouts},
    author={Ran Tian and Sisi Li and Nan Li and Ilya Kolmanovsky and Anouck Girard and Yildiray Yildiz},
    year={2018},
    eprint={1810.00829},
    archivePrefix={arXiv},
    primaryClass={cs.GT}
}

@article{RNN_predict_driver_intention,
author = {Zyner, Alex and Worrall, Stewart and Nebot, Eduardo},
year = {2018},
month = {02},
pages = {1-1},
title = {A Recurrent Neural Network Solution for Predicting Driver Intention at Unsignalized Intersections},
volume = {PP},
journal = {IEEE Robotics and Automation Letters},
}

@article{explicit_decision_tree,
author = {Li N and Chen H and Kolmanovsky I and Girard A.},
year = {2017},
title = {An Explicit Decision Tree Approach for Automated Driving},
volume = {1},
journal = {ASME. Dynamic Systems and Control Conference},
}

@article{TDM,
author = {Pong, Vitchyr and Gu, Shixiang and Dalal, Murtaza and Levine, Sergey},
year = {2018},
month = {02},
pages = {},
booktitle={ICLR},
title = {Temporal Difference Models: Model-Free Deep RL for Model-Based Control}
}

@InProceedings{MBMF_trajectory-centric,
  title =    {Combining Model-Based and Model-Free Updates for Trajectory-Centric Reinforcement Learning},
  author =   {Yevgen Chebotar and Karol Hausman and Marvin Zhang and Gaurav Sukhatme and Stefan Schaal and Sergey Levine},
  booktitle =    {Proceedings of the 34th International Conference on Machine Learning},
  pages =    {703--711},
  year =     {2017},
  volume =   {70},
  series =   {Proceedings of Machine Learning Research},
  month =    {06--11 Aug},
}

@misc{MBMF,
    title={MBMF: Model-Based Priors for Model-Free Reinforcement Learning},
    author={Somil Bansal and Roberto Calandra and Kurtland Chua and Sergey Levine and Claire Tomlin},
    year={2017},
    eprint={1709.03153},
    archivePrefix={arXiv},
}

@incollection{differentiable_MPC,
title = {Differentiable MPC for End-to-end Planning and Control},
author = {Amos, Brandon and Jimenez, Ivan and Sacks, Jacob and Boots, Byron and Kolter, J. Zico},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {8299--8310},
year = {2018},
publisher = {Curran Associates, Inc.},
}

@ARTICLE{MPC_VS_RL,
author={D. Ernst and M. Glavic and F. Capitanescu and L. Wehenkel}, 
journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)}, 
title={Reinforcement Learning Versus Model Predictive Control: A Comparison on a Power System Problem}, 
year={2009}, 
volume={39}, 
number={2}, 
pages={517-529},
month={April},
}

@article{MPC,
title = "Model predictive control: past, present and future",
journal = "Computers \& Chemical Engineering",
volume = "23",
number = "4",
pages = "667 - 682",
year = "1999",
author = "Manfred Morari and Jay H. Lee",
}

@ARTICLE{Gaussian_process, 
author={M. P. Deisenroth and D. Fox and C. E. Rasmussen}, 
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
title={Gaussian Processes for Data-Efficient Learning in Robotics and Control}, 
year={2015}, 
volume={37}, 
number={2}, 
pages={408-423}, 
month={Feb},
}

@incollection{Bayesian_optimization,
title = {Practical Bayesian Optimization of Machine Learning Algorithms},
author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
booktitle = {Advances in Neural Information Processing Systems 25},
%editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {2951--2959},
year = {2012},
publisher = {Curran Associates, Inc.},
}

@incollection{NIPS2015_5796,
title = {Learning Continuous Control Policies by Stochastic Value Gradients},
author = {Heess, Nicolas and Wayne, Gregory and Silver, David and Lillicrap, Timothy and Erez, Tom and Tassa, Yuval},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {2944--2952},
year = {2015},
publisher = {Curran Associates, Inc.},
%url = {http://papers.nips.cc/paper/5796-learning-continuous-control-policies-by-stochastic-value-gradients.pdf}
}

@incollection{policy_gradient_theorem,
title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
author = {Sutton, Richard S and David A. McAllester and Satinder P. Singh and Mansour, Yishay},
booktitle = {Advances in Neural Information Processing Systems 12},
editor = {S. A. Solla and T. K. Leen and K. M\"{u}ller},
pages = {1057--1063},
year = {2000},
publisher = {MIT Press},
%url = {http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf}
}

@inproceedings{TRPO_pre,
 author = {Kakade, Sham and Langford, John},
 title = {Approximately Optimal Approximate Reinforcement Learning},
 booktitle = {Proceedings of the Nineteenth International Conference on Machine Learning},
 series = {ICML '02},
 year = {2002},
 isbn = {1-55860-873-7},
 pages = {267--274},
 numpages = {8},
} 

@InProceedings{benchmarking_RL,
  title =    {Benchmarking Deep Reinforcement Learning for Continuous Control},
  author =   {Yan Duan and Xi Chen and Rein Houthooft and John Schulman and Pieter Abbeel},
  booktitle =    {Proceedings of The 33rd International Conference on Machine Learning},
  pages =    {1329--1338},
  year =   {2016},
}

@InProceedings{SNN4hrl,
  title =    {Stochastic Neural Networks for Hierarchical Reinforcement Learning},
  author =   {Carlos Florensa and Yan Duan and Pieter Abbeel},
  booktitle =   {Proceedings of The 34th International Conference on Machine Learning},
  year =   {2017},
}

@incollection{HIRO,
title = {Data-Efficient Hierarchical Reinforcement Learning},
author = {Nachum, Ofir and Gu, Shixiang (Shane) and Lee, Honglak and Levine, Sergey},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {3303--3313},
year = {2018},
% publisher = {Curran Associates, Inc.},
% url = {http://papers.nips.cc/paper/7591-data-efficient-hierarchical-reinforcement-learning.pdf}
}

@inproceedings{feudal,
title={FeUdal Networks for Hierarchical Reinforcement Learning},
author={Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
booktitle={International Conference on Machine Learning},
pages={3540--3549},
year={2017}
}

@inproceedings{categorical_gradient,
title = {Categorical Reparameterization with Gumbel-Softmax},
author  = {Eric Jang and Shixiang Gu and Ben Poole},
year  = {2017},
booktitle={International Conference on Learning Representations},
}

@InProceedings{HAC,
  title =    {Learning Multi-level Hierarchies with Hindsight},
  author =   {Andrew Levy and Robert Platt Jr. and Kate Saenko},
  booktitle =    {Proceedings of The 33rd International Conference on Machine Learning},
  year =   {2018},
}

@misc{sensitive_to_goal_space,
    title={Hierarchical Policy Learning is Sensitive to Goal Space Design},
    author={Zach Dwiel and Madhavun Candadai and Mariano J. Phielipp and Arjun K. Bansal},
    year={2019},
    eprint={1905.01537},
    archivePrefix={arXiv},
    % primaryClass={cs.LG}
}

@ARTICLE{DIYAN,
       author = {{Eysenbach}, Benjamin and {Gupta}, Abhishek and {Ibarz}, Julian and
         {Levine}, Sergey},
        title = "{Diversity is All You Need: Learning Skills without a Reward Function}",
      journal = {arXiv e-prints},
         year = "2018",
        month = "Feb",
}

@ARTICLE{Learning_and_Transfer_of_Modulated_Locomotor_Controllers,
       author = {{Heess}, Nicolas and {Wayne}, Greg and {Tassa}, Yuval and
         {Lillicrap}, Timothy and {Riedmiller}, Martin and {Silver}, David},
        title = "{Learning and Transfer of Modulated Locomotor Controllers}",
      journal = {arXiv e-prints},
         year = "2016",
        month = "Oct",
}

@inproceedings{MLSH,
title={{META} {LEARNING} {SHARED} {HIERARCHIES}},
author={Kevin Frans and Jonathan Ho and Xi Chen and Pieter Abbeel and John Schulman},
booktitle={International Conference on Learning Representations},
year={2018},
%url={https://openreview.net/forum?id=SyX0IeWAW},
}

@ARTICLE{goal-conditioned,
       author = {{Ghosh}, Dibya and {Gupta}, Abhishek and {Levine}, Sergey},
        title = "{Learning Actionable Representations with Goal-Conditioned Policies}",
      journal = {arXiv e-prints},
         year = "2018",
        month = "Nov",
}

@ARTICLE{goal_repr_learning,
       author = {{Nachum}, Ofir and {Gu}, Shixiang and {Lee}, Honglak and
         {Levine}, Sergey},
        title = "{Near-Optimal Representation Learning for Hierarchical Reinforcement Learning}",
      journal = {arXiv e-prints},
         year = "2018",
        month = "Oct",
}

@inproceedings{Sutton:1998_options,
title={Theoretical Results on Reinforcement Learning with Temporally Abstract Options},
author={Precup, Doina and Sutton, Richard S., and Singh, Satinder},
booktitle={European Conference on Machine Learning (ECML)},
publisher={Springer},
year={1998},
}

@article{Sutton:1999,
 author = {Sutton, Richard S. and Precup, Doina and Singh, Satinder},
 title = {Between MDPs and semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning},
 journal = {Artif. Intell.},
 volume = {112},
 number = {1-2},
 month = aug,
 year = {1999},
 issn = {0004-3702},
 pages = {181--211},
 numpages = {31},
 publisher = {Elsevier Science Publishers Ltd.},
}

@incollection{Tenenbaum2016NIPS,
title = {Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation},
author = {Kulkarni, Tejas D and Narasimhan, Karthik and Saeedi, Ardavan and Tenenbaum, Josh},
booktitle = {Advances in Neural Information Processing Systems 29},
pages = {3675--3683},
year = {2016},
publisher = {Curran Associates, Inc.},
}

@article{HRL_with_maxQ,
 author = {Dietterich, Thomas G.},
 title = {Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition},
 journal = {J. Artif. Int. Res.},
 volume = {13},
 number = {1},
 month = nov,
 year = {2000},
 issn = {1076-9757},
 pages = {227--303},
 numpages = {77},
 publisher = {AI Access Foundation},
} 

@incollection{Barto_HRL,
title = {Intrinsically Motivated Reinforcement Learning},
author = {Nuttapong Chentanez and Andrew G. Barto and Satinder P. Singh},
booktitle = {Advances in Neural Information Processing Systems 17},
editor = {L. K. Saul and Y. Weiss and L. Bottou},
pages = {1281--1288},
year = {2005},
publisher = {MIT Press},
}

@incollection{FUN,
title = {Feudal Reinforcement Learning},
author = {Dayan, Peter and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 5},
editor = {S. J. Hanson and J. D. Cowan and C. L. Giles},
pages = {271--278},
year = {1993},
publisher = {Morgan-Kaufmann},
}

@inproceedings{option-critic,
title={The Option-critic Architecture},
author={Pierre-Luc Bacon and Jean Harb and Doina Precup},
booktitle={AAAI},
pages={1726–1734},
year={2017}
}

@inproceedings{GAE,
title = {High-Dimensional Continuous Control Using Generalized Advantage Estimation},
author = {John Schulman and Philipp Moritz and Sergey Levine and Michael Jordan and Pieter Abbeel},
booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
year  = 2016
}

@incollection{HER,
title = {Hindsight Experience Replay},
author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Pieter Abbeel, OpenAI and Zaremba, Wojciech},
booktitle = {Advances in Neural Information Processing Systems 30},
pages = {5048--5058},
year = {2017},
publisher = {Curran Associates, Inc.},
}

@ARTICLE{HAAR,
       author = {{Wang}, Rui and {Li}, Siyuan and {Tang}, mingxue and
         {Zhang}, Chongjie},
        title = "{Hierarchical Reinforcement Learning with Advantage-Based Auxiliary Reward}",
      journal = {arXiv e-prints},
         year = "2019",
        month = "June",
}

@online{thuthesis,
  author =      {薛瑞尼},
  title =       {ThuThesis: 清华大学学位论文模板},
  urldate =     {2019-04-27},
  url =         {https://github.com/xueruini/thuthesis},
  year =        2017,
}
