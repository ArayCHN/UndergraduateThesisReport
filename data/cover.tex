\thusetup{
  %******************************
  % 注意：
  %   1. 配置里面不要出现空行
  %   2. 不需要的配置信息可以删除
  %******************************
  %
  %=====
  % 秘级
  %=====
%  secretlevel={秘密},
%  secretyear={10},
  %
  %=========
  % 中文信息
  %=========
  %ctitle={分层强化学习算法及其在机器人仿真环境中的应用研究},
  ctitle={HAAR:基于优势函数辅助奖励的分层强化学习算法},
  cdegree={工学学士},
  cdepartment={机械工程系},
  cmajor={机械工程},
  cauthor={王芮},
  csupervisor={雷丽萍副教授},
%  ccosupervisor={某某某教授}, % 联合指导老师
  % 日期自动使用当前时间，若需指定按如下方式修改：
  % cdate={超新星纪元},
  %
  % 博士后专有部分
%  cfirstdiscipline={计算机科学与技术},
%  cseconddiscipline={系统结构},
%  postdoctordate={2009年7月——2011年7月},
%  id={编号}, % 可以留空： id={},
%  udc={UDC}, % 可以留空
%  catalognumber={分类号}, % 可以留空
  %
  %=========
  % 英文信息
  %=========
%  etitle={An Introduction to \LaTeX{} Thesis Template of Tsinghua University v\version},
  % 这块比较复杂，需要分情况讨论：
  % 1. 学术型硕士
  %    edegree：必须为Master of Arts或Master of Science（注意大小写）
  %             “哲学、文学、历史学、法学、教育学、艺术学门类，公共管理学科
  %              填写Master of Arts，其它填写Master of Science”
  %    emajor：“获得一级学科授权的学科填写一级学科名称，其它填写二级学科名称”
  % 2. 专业型硕士
  %    edegree：“填写专业学位英文名称全称”
  %    emajor：“工程硕士填写工程领域，其它专业学位不填写此项”
  % 3. 学术型博士
  %    edegree：Doctor of Philosophy（注意大小写）
  %    emajor：“获得一级学科授权的学科填写一级学科名称，其它填写二级学科名称”
  % 4. 专业型博士
  %    edegree：“填写专业学位英文名称全称”
  %    emajor：不填写此项
%  edegree={Doctor of Engineering},
%  emajor={Computer Science and Technology},
%  eauthor={Xue Ruini},
%  esupervisor={Professor Zheng Weimin},
%  eassosupervisor={Chen Wenguang},
  % 日期自动生成，若需指定按如下方式修改：
  % edate={December, 2005}
  %
  % 关键词用“英文逗号”分割
  ckeywords={强化学习, 分层强化学习, 机器人学, 规划算法, 自动驾驶, 路径规划, 优化, 策略单调性},
  ekeywords={Reinforcement Learning, Hierarchical Reinforcement Learning, Robotics, Planning Algorithms, Self-Driving, Path Planning, Optimization, Monotonic Improvement}
}

% 定义中英文摘要和关键字
\begin{cabstract}
强化学习问题中的一个重要挑战是环境的稀疏奖励问题。如果环境反馈给智能体的信号过于稀疏，智能体的学习速度会非常缓慢。为此，人们提出分层强化学习的方法，来帮助强化学习算法提高采样效率，加速学习。在分层强化学习范式当中，特别地，我们需要为下层策略设计一个辅助奖励函数，从而促进下层策略的学习。在本篇论文中，我们提出了一个分层强化学习的框架，通过基于优势函数的下层奖励函数设计，它可以单调地同时优化上下层策略。截止目前，绝大多数最前沿的分层强化学习算法都对状态空间的表达非常敏感，或者依赖于手动设计的、只在特定环境下可用的下层奖励函数。得益于我们的辅助奖励函数设计只与优势函数的值相关这一特点，我们的算法不依赖于状态空间的表示方式，因此和其他方法相比更为通用和实用，尤其是在机器人连续动作空间这类应用场景。我们从理论上证明了这种算法可以保留上下层各自优化算法的单调特性，使整体策略也得到单调优化。实验方面，在Mujoco仿真环境的多个任务中，我们的算法表现均大大优于现有的最佳算法。最后，我们还搭建了一个自动驾驶仿真环境来验证本算法在更现实和复杂的环境下的表现。我们设计了迁移实验，将算法从Mujoco实验环境迁移到了自动驾驶环岛路径规划问题中，同时也指出了在这个环境中的下一步研究方向。
\end{cabstract}

\begin{eabstract}
One of the key challenges in reinforcement learning is related to sparse reward signals from the environment. Hierarchical reinforcement learning (HRL) can be utilized to solve the credit assignment problem in a sparse reward task. In the HRL paradigms, we need to design a low-level auxiliary reward function to facilitate low-level learning. In this paper, we propose an HRL framework which monotonically  optimizes the joint policy of the upper-level and lower-level  through the design of an advantage function-based auxiliary reward. Most state-of-the-art HRL algorithms are sensitive to state representations, or require careful design of the auxiliary reward functions  for low-level training. Our method is more generic and applicable in continuous control tasks since our auxiliary reward for low-level policy update is dependent only on the advantage function of the high-level policy, therefore invariant of the state representations. We theoretically prove that monotonic improvement of the joint-policy is guaranteed in our method. Experimental results show that our algorithm performs dramatically better than other state-of-the-art methods across multiple tasks in the Mujoco learning environment. Finally, we design a complex autonomous-driving simulation environment as a testbed for our algorithm in more realistic settings. We transfer our algorithm to this new environment and report the experiment results of our framework on the self-driving car's planning task at a roundabout, as well as point out the direction for future research.

\end{eabstract}

% 如果习惯关键字跟在摘要文字后面，可以用直接命令来设置，如下：
% \ckeywords{\TeX, \LaTeX, CJK, 模板, 论文}

% \ekeywords{\TeX, \LaTeX, CJK, template, thesis}
