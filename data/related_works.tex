%\usepackage{thuthesis}
%
%% 下面的包让argmax之类的，_{}的东西显示在argmax正下方
%\usepackage{amsmath}
%\DeclareMathOperator*{\argmax}{argmax}
%\DeclareMathOperator*{\argmin}{argmin}

% 定义所有的图片文件在 figures 子目录下
%\graphicspath{{../figures/}}

%\begin{document}

%%%% 封面部分
%\frontmatter
%\input{data/cover}
%% 如果使用授权说明扫描页，将可选参数中指定为扫描得到的 PDF 文件名，例如：
%% \makecover[scan-auth.pdf]
%\makecover

%% 目录
% 设置目录层级，三级小标题全部显示
%\setcounter{secnumdepth}{5}
%\setcounter{tocdepth}{5}
%\tableofcontents

%%% 正文部分
%\mainmatter
%\include{data/chap01}
%\include{data/chap02}
%\renewcommand\thesection{\arabic {section}} % use this command to disable chapter numbering

\chapter{外文资料调研阅读报告}

%TITLE need adjustments
\vspace*{5bp}
%{\centering \song\xiaosi 强化学习、免模型强化学习和分层强化学习}
%\vspace*{-10bp}

  \section{强化学习文献综述}
    在强化学习的著作\textit{Reinforcement Learning An Introduction}\cite{Sutton_book}中，Sutton对强化学习的理论进行了详尽的讨论。在这里，我们对强化学习的发展做一个精炼的综述，并从契合课题的框架进行一些讨论。另外，我们还将详细讨论强化学习的一些最新进展和它们在本课题中的应用。
    \subsection{问题描述}
      Sutton在1988年首次提出强化学习问题的一般描述\cite{Sutton_problem_formulation}。诸如学习走路、学习下棋的一般性问题都可以归为强化学习的问题。强化学习问题的四个基本元素是：一个策略（policy），一个奖励函数（reward），一个价值函数（value function）和一个环境模型（model）。例如，自动驾驶问题中，智能体面对它观察到的一个周边车流的\textbf{环境}，会给出一个输出，例如以某一加速度加速。在采取这一行动之后，智能体会得到一个\textbf{奖励}（可能有延迟），例如它到达了目的地，我们给它价值$100$的奖励。而如何决定加速度，就是我们想要智能体去学习的\textbf{策略}。在制定策略的过程中，智能体会去试图衡量目前所处状态的\textbf{价值}（价值函数），例如周围车辆非常拥堵，这个状态的价值就比较低，是我们希望避免进入的状态；而车前非常空旷则可能是我们赋予价值较高，希望进入的状态。\par
      在这里，我们将用一些研究者所默认的记号对我们的问题进行数学抽象。假设下标$t$表示任一时刻，而$S_t$代表某一时刻的状态，$A_t$表示智能体采取的行动，$\pi$表示智能体的策略，$R_t$表示智能体某一步获得的奖励。我们的目标是让智能体通过学习，获得尽可能大的总奖励，记为$G$。有时我们可以定义
      $$ G = \sum_{t} R_t $$
      但是多数时候我们会考虑加入一个折扣$\gamma$($0 < \gamma < 1$)，认为一个行为直接获得的奖励对它更有意义，而很久以后获得的奖励与这个行为的关联越来越小，记为
      \begin{align}
        G_{t_0} = \sum_{t = t0}^{t_n} \gamma^{t-t_0} R_t
        \label{eq:G}
      \end{align}
      \par 其中$t_n$要么是一个过程因为某种原因终止的时刻，要么是$\infty$。\par
      强化学习解决的是一个马尔可夫决策过程（MDP）\cite{Bellman_MDP}。在MDP中，$S_{t+1}$只取决于$S_t$和$A_t$，而不受到之前历史过程的影响。也就是说，$S_t$状态本身就包含了它所有的历史对后续过程的影响。在这个假设下，我们可以这样表达强化学习的目标\cite{DRL_for_driving}：记$v^{\pi}(s)$为状态$s$的价值函数。我们有$$ v^{\pi}(s) = E[G_{t_0} | S_{t_0} = s, \pi(s) ] $$
      $$ J(\pi) = \max_{\pi} E(v^{\pi}(s)) $$
      $$ \pi^*(s) = \argmax_{\pi} J(\pi) $$
    \par 强化学习的终极目标就是找到这个$\pi^*(s)$。Bellman在\textit{Dynamic Programming}一书中提出了著名的Bellman最优性等式\cite{Bellman_DP}
    \begin{align}
       v_*(s) = \max_{a} \sum_{s', r}p(s', r | s, a)[r + \gamma v_*(s')]
    \end{align}
    \par 同时，我们用$q(s, a)$表示在状态$s$下采取行动$a$可以获得的价值，那么Bellman最优性等式又可以表示为
    \begin{align}
       q_*(s, a) = \sum_{s', r}p(s', r | s, a)[r + \gamma \max_{a'} q_*(s', a')]
    \end{align}

    \par 这两个表达式在后续的推导中会非常有用。
    
    \subsection{典型算法综述}
      强化学习包含许多的算法，也有很多分类方法。其中，有一大类是列表方法，可以理解为是把$v(s)$保存起来的方法；另一大类是近似方法，也就是用一个函数去近似$v(s)$\cite{Sutton_book}。其中，第二类方法在近年来使用最多，神经网络使得函数可以近似非常复杂的映射关系，也让许多复杂问题通过强化学习得到了解决。\par
      列表式的强化学习方法包括动态规划\cite{Bellman_DP}，蒙特卡罗方法，时序差分学习（Temporal-difference learning，简称TD方法，包括Sarsa\cite{Sutton_book}, Q-learning\cite{Q_learning}）\cite{Sutton_problem_formulation}，多步自助法\cite{n_step_bootstrapping}（n-step bootstrapping），Dyna规划方法\cite{Sutton_book}等。\par
      TD方法是列表法强化学习的核心。这一族的算法有许多衍生版本，但其核心是利用经验不断更新价值函数$V(s)$，直至其收敛。更新的基本模式为
      $$ V(s) \gets V(s) + \alpha [r + V(s') - V(s)] $$
      \par 其中，$\alpha$表示函数更新的步幅。而当我们用$Q(s, a)$去替代$V(s)$的时候，算法就成为了Sarsa（state, action, reward, state, action \cite{deepRL_overview}）：
      $$ Q(s, a) \gets Q(s, a) + \alpha [r + \gamma Q(s', a') - Q(s, a)] $$
      \par 注意这里，$a'$是依据当前策略$\pi$采取的行动。而当我们把$Q(s', a')$替换成$\max_{a'} Q(s', a')$的时候，算法就成为了Q-学习法（Q-learning）：
      \begin{align}
      \label{eq:Q_learning}
        Q(s, a) \gets Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)]
      \end{align}
      \par 在这里，我们发现Sarsa和Q-学习的重要区别：Sarsa是同策略学习的（on-policy），估算$Q$值时采用的策略就是当前策略，而Q-学习是采用异策略（off-policy）学习的，估算$Q$值时采用的策略不是当前策略，而是最优策略。同策略与异策略也作为强化学习中的两种并行方法，本身并不改变算法架构，但是会影响收敛性等特性。许多算法都可以在这个维度上进行变异。\par
      注意到在以上的算法中，我们每一步都对$Q$或者$V$进行更新。在多步骤自助法（n-step bootstrapping）中，我们则将更新进行延迟。例如，某一行动结束，$n$步过后，我们才观察这一行为造成的影响，并且对它的值进行更新，其更新方式如下
      $$ V(S_t) \gets V(S_t) + \alpha(G_{t:t+n} - V(S_t)) $$
      \par 其中，$G_{t:t+n} = \sum_{i = 1}^{n}\gamma^{i-1}R_{t+i} + \gamma^n V(S_{t+n})$
      \par Sutton等人指出，多步骤自助法强化学习在训练时的表现往往比蒙特卡罗和TD方法都要更优\cite{Sutton_book}。
      \par 归功于深度强化学习的提出\cite{atari_2013, deepRL_overview}，近似函数的方法在近年来得到了极大的发展。近似函数有两种基本思路。第一种，是用某一类型的函数去近似状态的价值函数$v(s)$，从而帮助智能体根据不同的$v$采取不同的策略。第二种思路是跳过$v$的计算，而直接使用函数去近似一个策略$\pi$。在第二种思路中，$\pi(s)$的输出将直接是智能体应该采取的行为（或者行为的一个概率分布）。这种方法被称为``策略优化''，围绕策略优化问题衍生出一系列的策略梯度求解方法，成为近年来强化学习的热点研究方向之一。进一步的，在策略优化问题中，如果我们同时对价值函数$v(s)$进行近似和更新，那么可以得到一类框架，我们称之为actor-critic架构\cite{Sutton_book}。在后续的篇目中，我们会对此进行具体介绍。\par
      在深度强化学习中，比较著名的算法有DDPG\cite{DDPG}，A2C\cite{A3C}，TRPO\cite{TRPO}，ACKTR\cite{ACKTR}，ACER\cite{ACER}，PPO\cite{PPO}等。这些算法全部忽略了环境的模型，或者环境模型是学习得到的（TRPO）。还有一类算法，其中的环境模型是提前设计建模得到的而非习得，其中有代表性的就是AlphaZero\cite{AlphaZero}。这类算法也是我们想探究的方向——是否通过提前建模，能够让自动驾驶车辆在环岛这类复杂环境中有更好的表现？在接下来的章节中，我们会详细介绍其中有代表性，并且在我们的场景下易于使用的算法。\par
      另外，在对价值函数进行优化时（梯度下降），不同的算法可能带来不同的结果。A3C采用了RMSProp算法\cite{RMSProp}，我们往往还可以选择随机梯度下降法（SGD），带动量的随机梯度下降法\cite{SGD_momentum}等方法，在算法设计时需要进行尝试和选择。Sutton在书中对此进行了讨论\cite{Sutton_book}。此外，Ruder在2016的一篇综述文中对各种梯度下降算法进行了详尽的探讨\cite{GD_overview}。

      
    \subsection{无模型的深度强化学习}
      Mnih等人在2013年首度提出深度Q-网络（deep Q-network，简称DQN）\cite{atari_2013}，2015年，他们的另一篇论文同样描述了DQN\cite{nature2015}，激发了一轮深度神经网络与强化学习结合的热潮\cite{deepRL_overview}。DQN的基本思路是创造一个很一般的端到端的网络$Q(s, a | \theta)$，其中$\theta$是这个网络的参数。例如对于一个游戏，网络的输入直接就是未处理的图像，输出直接就是在某个阶段，游戏中的动作对应的价值。在传统Q-learning的框架下，DQN在每一步更新$q(s, a)$值的时候，不是直接使用\eqref{eq:Q_learning}，而是利用梯度下降法去更新网络$Q$的参数$\theta$，使得$Q$估值总损失最小
      \begin{align*}
        \bigtriangledown_{\theta_i}Loss(\theta_i) = E_{s, a, s'}[(r+\gamma \max_{a'}Q(s', a'|\theta_{i-1}) - Q(s, a|\theta_i))\bigtriangledown_{\theta_i}Q(s, a|\theta_i)]
      \end{align*}
      这里DQN的网络结构参考了Hinton等人提出的AlexNet\cite{AlexNet}，也就是典型的卷积神经网络（CNN）结构。DQN的最大贡献在于，由于使用了经验回放的方法，DQN可以收敛，从而使得深度神经网络在强化学习中的应用成为了可能。由于神经网络的高度非线性，DQN得以模拟足够复杂的场景。DQN在Atari系列游戏中取得了突破性的进展，甚至在一些游戏中超越了人类专家的水平。\par
      不过由于我们想探究的问题更多地涉及到低维度的传感器信息输入，而不是高维度的图像直接输入，因此DQN一类的方法对我们的帮助并不是很大。而策略梯度方法则对我们有很大的借鉴意义，因此我们也将集中讨论这一类的方法。\par
      前面我们提到，在策略优化问题中，有一类架构被称为actor-critic架构。Mnih等人在2016年提出的A3C（asynchronous advantage actor-critic）算法\cite{A3C}成为应用actor-critic架构的经典案例。A3C算法同时维护一个策略函数$\pi(a|s; \theta)$和一个价值函数的估计$V(s; \theta_v )$，这两个函数均为卷积神经网络CNN，并且两个网络除了输出层以外的层均是共享的。其中， $\pi(a|s; \theta)$称为actor，而$V(s; \theta_v )$称为critic。actor-critic会计算一个``优势''，定义为$\delta = R + \gamma V(S'; \theta_v) - V(S; \theta_v)$\cite{Sutton_book}。A3C借鉴了多步骤自助法的思路（设步数为$k$），在计算优势时，变异为$\delta_t = \sum_{i = 0}^{k - 1}\gamma^i r_{t+i} + \gamma^k V(S_{t+k}; \theta_v) - V(S; \theta_v)$。每一步，我们依然对$\theta$和$\theta_v$进行更新，梯度的计算方法如下（注意计算梯度以后，还需要选择一个梯度下降方法对$\theta$和$\theta_v$进行更新）
      \begin{align}
      \label{eq:A3C_gradient1}
          d\theta &= \delta_t \bigtriangledown_{\theta} log_\pi(A_t|S_t; \theta)\\
      \label{eq:A3C_gradient2}
          d\theta_v &= \delta_t \bigtriangledown_{\theta_v} \delta_t^2
      \end{align}
      \par 如果我们采用多个智能体并行训练，把它们各自的梯度叠加在一起（也就是累加每个智能体各自求得的\eqref{eq:A3C_gradient1}\eqref{eq:A3C_gradient2}式），最后统一更新$\theta$和$\theta_v$，就称之为异步训练方法，这种算法也因此被称为asynchronous advantage actor-critic。如果条件受限没有异步，而只是用一个智能体进行训练，这个算法就称为A2C（advantage actor-critic）。actor-critic架构被认为是能够有效加速训练的，因此越来越多的算法都基于这一架构进行设计。\par
      2015年，Schulamn等人提出了经典的（Trust Region Policy Optimization, TRPO）算法\cite{TRPO}。在文章中，他们首先从理论上证明了一种策略迭代更新的MM（minorization-maximization）算法，可以确保策略对应的总价值（$\eta(\pi) = E_{s_0}[\sum \gamma^t r(s_t); \pi_\theta]$）单调不下降。依据这一理论基础，他们进行了多步近似，将MM算法中的优化（求$\argmax$）的问题转化为了一个更简单的加限制的优化问题（信任空间优化，trust-region optimization）。在进行进一步近似后，优化式中的期望形式可以用蒙特卡罗方法进行估算。最后，优化式本身则利用一种近似的方法进行优化。在实验中，他们采用了神经网络作为策略$\pi$的表达。TRPO的更新方法是：
      \begin{align}
        \begin{aligned}
          \theta = \argmax_\theta E\bigg[\frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)}Q_{\theta_{old}}(s, a)\bigg]\\
          \text{subject to } E[D_{KL}(\pi_{\theta_{old}}(\cdot|s)||\pi_\theta(\cdot|s))] \leq \delta
        \end{aligned}
      \end{align}
      \par 其中，$\delta$是一个认为选定的小值。注意，在论文的附录中，作者提出采用一阶近似对上式进行求解，优化问题就可以就变为一个典型的拉格朗日共轭问题\cite{Convex_optimization_book}。不过，由于TRPO只是保证了每一步更新，策略都变得更好，却没有考虑探索性，因此，TRPO的结果很容易困在局部最优值。在Schulman之后，Wu等人于2017年又提出了一个改进算法ACKTR\cite{ACKTR}（Actor Critic using Kronecker-Factored Trust Region），采用actor-critic架构，并利用Kronecker因子去近似梯度下降。同年，Schulman等人又提出了PPO算法\cite{PPO}，在实现难度和算法表现方面取得了平衡。PPO算法的基本思路有三点：
      \begin{itemize}
        \item 去除TRPO的优化限制条件，在TRPO的基础上，采用一个clip函数，从而阻止$\theta$每一次更新太大，确保收敛
        \item 在优化的函数里，通过引入系数的形式，同时优化actor和critic，整个优化过程只有一个优化目标函数
        \item 通过在优化目标中加入一个熵奖励，增加算法的探索性
      \end{itemize}
      \par PPO把强化学习问题转化为一个形式简单的单一函数优化问题，在许多方面取得了比以往算法（A2C, ACER, TRPO, etc.）更优的表现。\par
      在2015年，Silver首度提出了针对连续动作空间的确定性算法DPG（deterministic policy gradient）\cite{DPG}。与确定性算法对应的是随机算法，也就是策略是行动的概率分布。在高维的动作空间内（例如动作是连续的），DPG比与之对应的随机算法获得了更优的结果。DPG采用actor-critic架构，并且是异策略（off-policy）训练的。注意这里，DPG的actor-critic架构与Sutton书中提到的actor-critic架构有所区别。它是不以价值函数作为基准的，而正常的actor-critic架构应该以价值函数作为基准，计算优势，再进行求导\cite{Sutton_book}，参见前面提到的A3C算法\cite{A3C}。同时，DPG算法采用的是Q-learning结构，不是用$V(s)$对价值进行估计，而是用$Q(s, \pi(s))$对价值进行估计。由于算法本身是确定性的，DPG与随机异策略actor-critic架构\cite{off-policy_actor-critic}不同，不需要针对策略进行重要性采样（importance sampling，在\cite{Sutton_book}中有详细讨论）。DPG本身没有选择神经网络，而是可以采用一般的函数，尤其是线性函数。\par
      DPG算法对$\pi(s)$和$\mu(s)$的更新方法如下：
      \begin{align}
        \begin{aligned}
          &\delta_t = r_t + \gamma Q(S_{t+1}, A_{t+1};\theta_Q) - Q(S_t, A_t;\theta_Q)\\
          &\theta_Q \gets \theta_Q + \alpha_{\theta_Q}\delta_t \bigtriangledown_{\theta_Q}Q(s, a;\theta_Q)\\
          &\theta_\mu \gets \theta_\mu + \alpha_{\theta_\mu}\bigtriangledown_{\theta_\mu}\mu(s;\theta_\mu)\bigtriangledown_{a}Q(s, a;\theta_Q)|_{a = \mu(s;\theta_\mu)}
        \end{aligned}
        \label{eq:DPG_update}
      \end{align}
      \par 基于DPG，Lillicrap等人提出的改进算法，DDPG算法\cite{DDPG}，则全面采用了深度神经网络。为了能够让训练收敛，DDPG使用了一个目标网络（target network）的概念。目标网络的定义是缓慢地跟踪依据梯度下降优化得到的网络参数$\theta_Q$和$\theta_\mu$，$\theta_Q$是状态行动对的价值估计网络$Q$的参数，而$\theta_\mu$是actor-critic架构中actor采用的策略的参数。目标网络参数$\theta_Q'$和$\theta_\mu'$的更新方法如下（在每一步迭代过后进行这个更新）：
      \begin{align*}
        \theta' = \tau \theta + (1-\tau) \theta', \tau \ll 1
        \label{eq:target_network}
      \end{align*}
      \par 这样，相当于强迫目标网络缓慢地变化，从而增强了训练的收敛性。目标网络主要用于带入\eqref{eq:DPG_update}中$\delta_t$的计算。同时，DDPG采用了经验回放的方法，在每一步迭代中都采用之前历史中一批随机挑选的过往经验进行批量计算，而非一步的结果，这样可以确保网络训练时不带有时序偏差。另外，DDPG没有像DPG一样采用异策略学习，而是在策略基础上加上一个噪声分布以确保``探索''性。\par
    
    \subsection{有模型的深度强化学习}
      有模型的深度强化学习问题可以分为模型是``习得''的和模型是给定的两种。在这里，我们首先关注一下给定模型的强化学习的应用。
      \par 在模型给定的算法中，最著名的例子就是2016年提出的AlphaGo\cite{AlphaGo}。Deepmind在2017年进一步提出了AlphaZero\cite{AlphaZero}，其算法和AlphaGo相比有几大特点：
      \begin{enumerate}
        \item AlphaZero完全采用逐步迭代的方式，而AlphaGo在训练中会挑选之前出现过的最好策略
        \item AlphaZero拥有的先验知识只有棋盘的格局和下棋的规则，并且这些知识只用来进行模拟
        \item AlphaZero对参数的变动更加鲁棒，不同棋类游戏用的都是同一套参数
        \item AlphaZero在训练中没有对输入进行任何数据加强的操作，而AlphaGo则对棋盘进行了镜像操作
      \end{enumerate}
      \par AlphaZero采用蒙特卡罗树状搜索（Monte Carlo Tree Search, MCTS\cite{Sutton_book}）的方法，每次优先访问训练历史中访问次数低，但访问概率$\pi(a|s)$应该较高，且$Q(s, a)$的状态行为对$(s, a)$。同时，在此基础上加上一个噪声（在AlphaZero的例子中，一个狄利赫里分布的噪声），确保探索性。AlphaZero采用深度卷积神经网络作为策略$\pi(s)$的拟合函数，其输出为$\pi(s) = (\mathbf{p}, v)$，其中，$\mathbf{p}$为在某个状态下选择各个行为的概率，$v$为根据$s$推测的状态估值,$v = \mathbb{E}[z]$（胜负平局，$z$的值分别为$+1, -1, 0$）。针对该网络定义的损失函数为
      \begin{align}
        L = (z-v)^2 - \pi^T log \mathbf(p) + \lambda ||\theta||^2
      \end{align}
      \par 可以看出，这个损失函数试图使$v$的预测和棋局结果$z$更加接近；$\pi$代表了训练中各个动作的搜索概率，损失函数试图让神经网络的输出$\mathbf(p)$与之尽可能接近。
      \par 这里所谓“给定模型”，其实就是环境是已知的，不需要真的和环境交互，而是在仿真里直接进行下棋这个活动即可。然而，这也意味着这个问题本身与直接和未知环境交互并没有区别。尽管模型给定，算法并没有从模型本身得出什么规律，因此这个模型本质上就是一个环境。\par
      在强化学习问题中，从与环境的交互历史中逐渐学习到模型，则是另一个有趣的思路。在\cite{Sutton_book}中，Dyna架构就是这样一种思路。在强化学习中，利用模型去近似求解$v(s)$是一个``规划''（planning）问题。传统的列表式方法可以根据训练中得到的环境反馈不断更新模型。2015年，Oh等人在\cite{action-conditional_prediction}一文中提出了一种利用神经网络学习高维度环境的方法，使得复杂环境的模拟也成为了可能。\par 
      基于\cite{action-conditional_prediction}的结果，Racani\`{e}re等人在论文\cite{I2A}中提出了一种利用模型去增强无模型强化学习的方案，简称为I2C算法。这种方法本质上是细化了的A3C算法。注意到A3C本身是一个无模型的方法，它仅仅依赖于$\pi(a|s)$和$v(s)$的估计（回顾一下\eqref{eq:A3C_gradient1}\eqref{eq:A3C_gradient2}两个更新式）。但是，I2C算法修改了$\theta$的组织形式，构建了一个较为复杂的网络结构，因而引入了环境模型。其基本架构如图\ref{fig:I2A}所示。
      \begin{figure}[H] % use float package if you want it here
        \centering
        \includegraphics{I2A}
        \caption{I2A的基本结构：a) 一个预先训练好的神经网络，可以预测在策略$\hat{\pi}$下，状态$s$的一系列时序变化；b) 一个子网络，用于``理解''a)的输出结果；c) 整体架构}
        \label{fig:I2A}
      \end{figure}
      \par 图\ref{fig:I2A}c)展示了网络整体架构，可以看到整体网络的输入为$o_t$，也就是对状态$S_t$的观测，输出为策略$\pi$和价值函数估值$V$（用于actor-critic架构）。因此，这个结构满足A3C的基本框架。在框架内部，网络结构采用了含有预测模型网络处理的输入，也采用了无模型网络处理的输入，分别作为两个分支，最后通过一个子网络汇合。具体的结构参数可以参见论文\cite{I2A}的附录。通过引入这个特殊的网络结构，I2A算法将习得的模型结合进了原有的无模型学习框架中。同时，实验结果表明，这里的模型可以有错误和不精确的地方，学习结果依然是鲁棒的。
  
  \section{部分可观测的马尔可夫决策过程}
    我们要解决的问题是典型的部分可观测马尔科夫决策过程。当在马尔可夫过程中，状态$s$不能被完全观测时（例如汽车只能看到自己视野范围内的几辆车），我们称这个问题为部分可观测马尔科夫决策过程（partially observable Markov decision process, POMDP）。POMDP在强化学习社区中被当做一种特殊的问题看待，也围绕这类问题衍生出许多解决方案。2015年，Hausknecht等人提出了DRQN\cite{DRQ}（deep recurrent Q network），把它应用于经过修改的部分可观测的Atari游戏上并取得了比DQN更好的结果。\par
    DRQN利用了循环神经网络（RNN，recurrent neural network）和LSTM\cite{LSTM}（long short term memory）的概念，用这样一层LSTM神经元替代DQN网络全联通层的第一层。在我们的设计中，LSTM也被用来解决状态部分可观测的问题。
  
  \section{有模型和无模型结合的高效强化学习}
    有模型强化学习和无模型强化学习有各自的优缺点和适用场合。例如，有模型强化学习可以大大提高样本利用率，但是如果模型有偏差，会造成习得错误的策略，对于一些特别复杂的问题，很难得到精准的模型。无模型的强化学习可以应用在复杂场景，但是样本利用率很低，在一些容易规范化的问题中不宜采用。因此我们考虑在环岛路径规划中结合两者的优势。\par
    2017年，Bansal等人提出MBMF算法（Model-Based Priors for Model-Free RL）\cite{MBMF}。记策略为$\pi_\theta$，代价函数为$J = -G_{t:t+n}$。MBMF最终想要得到的是代价函数$J$和策略参数$\theta$的关系$J(\theta)$，从而优化策略的参数。MBMF的算法如下：首先根据观察，训练出系统的动力学特性，记为一个函数$s_{t+1} = f(s_t, a)$（模型）。接下来，利用$f$，可以预测在某个策略$\pi_\theta$下，对应的奖励值期望（原文称之为代价函数$J$），记为$J_{MB}(\theta)$。这是基于模型得到的代价函数估计值。考虑到这个模型存在偏差，MBMF的MF部分把$J_{MB}(\theta)$作为初值，利用观察到的数据对$(\theta, J(\theta))$，进行拟合函数优化，优化得到最终的函数$J_{MF}(\theta)$。特别地，在上述拟合函数$f, J_{MB}, J_{MF}$当中，他们没有使用神经网络，而是使用了高斯过程\cite{Gaussian_process}（gaussian process, GP）和贝叶斯优化\cite{Bayesian_optimization}（Bayesian optimization, BO）的方法。MBMF的论文认为，通过引入模型，提高了采样效率，加速了学习过程，通过无模型的训练部分，避免了模型的偏差问题。从而两者的优势被结合起来。因此，MBMF算法的关键在于，把$J_{MB}$作为贝叶斯优化的一个``初值''。与Dyna架构\cite{Sutton_book}不同，MBMF的MF部分只采用真实环境里采集的数据集$(\theta, J(\theta))$对拟合函数$J_{MF}$进行优化，而不像Dyna架构利用学习到的模型去生成了大量的合成数据。\par
    2018年，Pong等人提出了结合有模型和无模型的一种训练方法时许差异模型（temporal-difference model, TDM）\cite{TDM}。之前的一些论文通过目标条件价值函数（goal-conditioned value function）$Q(s, a, s_g)$的定义，将无模型强化学习（仅学习$Q$值）和有模型强化学习（需要学习状态转移概率分布$p(s_{t+1}|s_t, a)$）被联系在了一起。其中，$s_g$表示某一目标状态。如果定义奖励函数$r(s_t, a, s_{t+1}, s_g) = -D(s_{t+1}, s_g)$，其中，$D$为一个表示状态间距离的函数，那么$Q(s, a, s_g)$最终将收敛于$-D(s_{t+1}, s_g)$。$Q(s, a, s_g)$表示的含义是就是从$s$转移到$s_g$的能力，因此包含了模型的信息，但同时，这个方法没有显式地去学习状态转移函数，无模型强化学习方法能够用的地方它也都能用。TDM的贡献在于引入了时序差异的目标条件价值函数，$Q(s, a, s_g, \tau)$，表示在$\tau$步之内，状态$s$采取行动$a$后，再通过某种方式转移到$s_g$的能力。因此，$Q$的定义（和更新方法）为
    \begin{align}
      Q(s_t, a_t, s_g, \tau) = \mathbb{E}_{s_{t+1} \sim s_t, a}[-D(s_{t+1}, s_g)1[\tau = 0] + \max_a Q(s_{t+1}, a, s_g, \tau - 1)1[\tau \neq 0]]
    \end{align}
    \par 和传统强化学习对$G$的定义\eqref{eq:G}不同，TDM算法不采用折扣$\gamma$。$\tau$的引入也让TDM算法能够进行稀疏采样，每隔$K$个时间才对奖励$r$进行一次累积，从而加快训练速度。
    \begin{align}
    \begin{aligned}
      & a_t = \argmax_{a_{t:K:t+T}, s_{t:K:t+T}} \sum_{i = t, t+K, ..., t+T} r(s_i, a_i)\\
      \text{subject to } &Q(s_i, a_i, s_{i+K}, K-1) = 0 \text{,   } \forall i \in \{t, t+K, ..., t+T-K\} 
      \label{eq:TDM_k}
    \end{aligned}
    \end{align}
    \par $K$最终可以扩大到$T$，也就是，不再考虑过程中其余时刻的奖励函数值，直接用$T$时刻的奖励函数值作为总价值。这在一些路径规划问题中是非常有用的（例如，奖励函数非常稀疏，只有在最终到达目的地时才出现一个大的奖励）。
    \par 注意到\eqref{eq:TDM_k}是一个有限制的优化问题，为了避免解这样一个问题，TDM的论文在实际操作中采用了一些构造$Q$函数形式的技巧，具体内容参加论文\cite{TDM}，不在此详细讨论。不过，TDM的一个限制是它只适用于结果导向的问题，例如一个任务的目标是到达一个地点，而收获的奖励和到达这个点过程中的路径细节无关。因为只有这样，才可以采用稀疏采样获取的奖励值作为总的奖励，来优化$a_t$的选择。
  
  \section{分层强化学习}
    多任务和分层强化学习在正文中已有详细的描述。为完整性，此处附上。

分层强化学习是用于解决复杂强化学习问题和稀疏奖励问题的常见手段\cite{Sutton:1999, HRL_with_maxQ, FUN, Barto_HRL}。选择框架\cite{Sutton:1998_options, Sutton:1999}是分层强化学习中的一个常见框架，而基于选择框架的Option-Critic结构\cite{option-critic}则为早期的HRL的端到端训练提供了重要的理论基础。然而Option-critic当中，上层策略是一个简单的$\epsilon$-贪心策略，也就是基于价值的迭代算法，不属于策略梯度算法，这也限制了这个算法在复杂场景的应用前景。

近两年来分层强化学习的研究聚焦于连续动作空间。应用于离散空间问题的HRL由于对``目标''的定义更为简单，因此算法无法拓展到连续空间。在本篇论文中，连续动作空间将是我们解决的主要问题。连续空间的分层强化学习研究可以主要分为两个不同的思路，一个是下层基于上层目标的强化学习，另一个是下层基于上层选择的强化学习。特别地，Option-critic虽然不是针对连续动作空间的算法，但它属于第二种方案。它同时训练上层（option）和下层（actor）策略。接下来我们将分别介绍这两类思路。

\textbf{子目标设计}

其中一个思路是，训练一个上下层的策略结构，上层策略需要给下层策略提供一个子目标（subgoal），下层策略朝向这个子目标去走，并且依据与自身状态和子目标相关的某个函数，而获得一个内部反馈（intrinsic reward，或者说辅助反馈，auxiliary reward），而上层策略则获得环境给出的真实反馈（extrinsic reward）。许多算法都采用了这个思路\cite{Sutton:1999, Tenenbaum2016NIPS, HIRO, HAC, feudal}。

这个产生内部反馈的函数，有不同的选择。例如在高效分层强化学习（HIRO\cite{HIRO}）算法中，这个函数是下层状态与智能体目标状态的欧氏距离。在HAC\cite{HAC}算法中，训练场景根据智能体状态是否达到子目标确定，而``达到子目标''的判据则也是欧氏距离。而在层级制网络强化学习算法（FeUdal Network\cite{feudal}，其主要受到\cite{FUN}这篇论文的启发）当中，这个函数是下层行动方向与上层给出目标方向的余弦距离。同时在FeUdal Networks这篇论文当中，下层策略也获得了外界环境的奖励，这与内部反馈一起加权处理后成为底层策略获得的总的奖励，亦即$R_{low} = R_{extrinsic} + \alpha R_{intrinsic}$。注意到在分层强化学习的这种思路当中，上下层策略都是一起从零开始训练的。

这些算法共同的问题是需要利用函数来衡量特定的状态空间差距。常用的欧氏距离和余弦距离本质上都是把状态空间当作欧几里得空间，这要求，（1）状态空间的各个维度的数量级相似，具有可比拟性，（2）欧氏距离可以表征智能体状态和预期状态的差距。其中第（2）点也可以用数学语言翻译成，如果对于状态$S_1, S_2$和$S_3$，$d(S_1, S_2) > d(S_1, S_3)$，则状态$S_3$比$S_2$和$S_1$相似。这一特征在状态表征为观测值时，是无法保证的。2019年的最新论文\cite{goal-conditioned}中也论证了这个问题。一些算法试图解决这个问题。它们的思路是不再使用状态的欧氏距离作为状态差异的表征，而采用一种特殊的``表征''（representation），将状态空间投影到一个新的空间里。例如Ghosh等人在2019年的论文\cite{goal-conditioned}采用了状态对应的行动（actionable representation）的欧氏距离；Nachum等人在2018年\cite{goal_repr_learning}提出了对目标的表示进行学习的方法，学习一个投影$f(S)$，下层策略获得上层给定的特定目标$g$，并结合自身观察值而决定采取的行为。在下层训练时，奖励值由$f(S)$与$g$的欧氏距离决定。

\textbf{下层预训练}

另一种思路是上层策略输出对下层策略的一个选择，因而上下层策略充分解离，常用对上下层策略分开训练\cite{DIYAN, SNN4hrl, Learning_and_Transfer_of_Modulated_Locomotor_Controllers}。下层策略的训练往往依赖于特定的应用域知识，有的算法会加上奖励多样性（熵）的方式。这种算法的优势在于，由于底层策略是共享的，一方面同一套底层策略可以用于多种不同的具体任务，另一方面在特定任务的训练中，由于底层策略已经比较成熟，因而可以大大加速学习过程。

在基于随机神经网络的分层强化学习算法（SNN for HRL\cite{SNN4hrl}）论文中，作者们提出了一种典型的分层架构：利用随机神经网络构建上层策略，输出1-hot向量表示的隐式编码（latent code）（一个一维向量$[x_1, x_2, ..., x_n]^T$，其中只有一个元素为$1$，其余均为$0$），来表示选择哪一个下层策略。下层策略的输入为环境的观察值$o_t$和1-hot向量$v$，输出值是智能体动作空间的行动。为了获得可用的下层策略，SNN for HRL要求进行底层策略预训练。预训练法是分层强化学习当中常用的一种算法。它的基本思路是，在一个比较简单的、环境反馈（即奖励值）比较密集的环境中训练下层策略。由于实际策略在操作的时候不需要获得``奖励''，因此只要观测空间和动作空间的维度相同，下层策略就可以直接在不同环境中迁移。SNN的一大贡献在于，为了能够训练出需要1-hot向量输入的下层策略，引入了一个共享信息熵的奖励（Mutual-Information, MI bonus）。2017年提出的MLSH算法\cite{MLSH}也需要预训练，不过它在预训练阶段同时训练了上下层策略。为了解决上层策略动态的问题，这个算法限制上层策略只参与若干次训练，之后都集中训练下层策略。底层策略集在新场景应用的时候，则完全不训练底层策略。

正如SNN一样，为了能够获得复用性强的下层策略，许多研究聚焦于提升预训练策略的多样性（diversity）\cite{DIYAN, Learning_and_Transfer_of_Modulated_Locomotor_Controllers}。例如DIYAN（diversity is all you need）算法甚至只考虑多样性而不接受环境的奖励。有的算法\cite{Learning_and_Transfer_of_Modulated_Locomotor_Controllers}为了获取较好的底层策略，需要手动设计一些简单的预训练场景来``引导''底层策略学习，这也是我们所不希望的。这类现有预训练算法的共同问题在于，下层策略一旦训练出来，在HRL的框架下就不可以再改变（参数被``冻结''）。这是有较大问题的。例如，SNN文章中提出，在蚂蚁行走的环境中，如果下层策略无法改变，那么在上层策略切换下层策略的时候，蚂蚁会很容易摔倒。由此我们联想到，能否在训练上层策略的同时修正下层策略，来提高整体策略的表现？SNN文章中作者指出，有一种潜在的方法可以联合训练上下层策略，而这种方法是基于对整体网络的梯度下降\cite{categorical_gradient}。然而，正如FeUdal Network\cite{feudal}一文指出的，这样一类直接对整体网络结构进行上下层一起优化的算法可能会丢失上层输出的语义信息，另外，这类算法最终容易退化成（i）上层策略始终只选择一个底层策略（ii）上层策略不断重新选择底层策略。这两种退化模式都等效于丢失了分层结构。因此，我们的方法决定采用上下层同时训练，但分成两次优化。这样，我们可以确保上下层策略具有足够的解离度，分层结构得以保留，上下层输出动作的含义都是可解释的。
    
  \section{强化学习和模型预测控制的对比}
  模型预测控制\cite{MPC}（MPC, model-predictive control）是一种最优控制的算法，主要利用模型对确定性离散时间系统进行控制。MPC和RL有许多相似之处。MPC和RL的主要区别在于，RL会``学习''一个模型或者不显式地利用模型，而MPC则利用模型进行预测，从而进行最优化\cite{MPC_VS_RL}。\par
  MPC的方法如下：假设环境的模型为$s_{t+1} = f(s_t, a_t)$，每次规划后面$n$步的行动序列$a_t, a_{t+1}, ..., a_{t+n-1}$，选取最优策略序列的头一个行动$a_t^*$作为最优行动。
  \begin{align}
  \begin{aligned}
        a_t^* = \argmax_{a_t, s_{t+1}, a_{t+1}, s_{t+2}, ... , a_{t+n-1}, s_{t+n}} G_{t:t+n-1}
    \text{  subject to } s_{t+i+1} = f(s_{t+i}, a_{t+i})
  \end{aligned}
  \end{align}
  \par 在RL的研究中，有不少利用/结合MPC的算法，可以结合两者的优势，更为稳定。
  
  \section{强化学习在自动驾驶领域的应用}
    目前对于自动驾驶在环岛环境下的路径规划这一特定问题，尚没有采用强化学习解决的方案。2018年，Zyner等人提出了利用RNN估计环岛车辆目的地的方法\cite{RNN_predict_driver_intention}。他们的工作采用了深度学习的算法，但是只是涉及预估，并没有涉及具体策略的制定。\par
    2017年，Li等人提出了一种基于搜索树的自动驾驶决策算法\cite{explicit_decision_tree}。他们的算法和强化学习类似，不过是通过依次设定初始状态，之后采用搜索+剪枝的方法来完成对$Q(s, a)$的估计，并记录在表格里面。这种方法比较平凡，并且只能应对观测空间较小，并且动作空间离散的问题，因此实际应用能力比较有限。\par
    2018年，Ran等人使用博弈论的方法对这个问题进行了探索\cite{game_theory}。他们采用的算法类似于强化学习，但也有所不同。在这篇文章中，决策过程利用了博弈论的方案。作者将车辆的驾驶人员分为保守和激进两种类型，在驾驶过程中，智能体自主车辆（ego vehicle）会不断依据对方司机的行为更新对对方车辆（opponent vehicle）分类的估计（通过神经网络完成）。完成这个估计以后，智能体会采取博弈的方法，每次采用某一行动，让对方采取它策略下的行动，由此预测对方在此后$n$步的行为。通过暴力搜索，可以得到最优的一个行动序列$A_t, A_{t+1}, ..., A_{t+n-1}$。通过大量的训练，作者拟合了一个神经网络去存储状态$S_t$到最优动作$A_t$的映射，从而在实际操作中避免了搜索（离线训练）。
    \begin{figure}[H] % use float package if you want it here
        \centering
        \includegraphics[scale=1.5]{game_theory_structure}
        \caption{论文\cite{game_theory}提出的基于博弈论的决策方案整体框架。}
        \label{fig:game_theory_structure}
    \end{figure}
    \par 图片\ref{fig:game_theory_structure}展示了基于博弈论的算法的整体框架。其整体流程如下：首先，某一状态输入神经网络B，判断其是否属于``关键状态''（义为，这个状态下两辆车是否已经足够接近，开始相互影响），如果是，则进入下一步，通过神经网络C依据对方车辆的行为来判断对方车辆的类别是保守还是激进；判断结束后，将判断结果和当前状态一起输入神经网络A，从而获取当前状态下应采取的最优行动。这其中的神经网络都是依据上面提到的博弈论算法来进行预训练的。\par
    这个算法本质上是一个有监督学习算法，通过大量的训练``记忆''每个场景下应该采取的策略。和强化学习的区别在于，此基于博弈论的算法没有对状态和行动的价值进行估计，同时，由于没有获取环境的实际反馈（例如在仿真环境中，训练实际是否成功，并没有作为神经网络训练的输入），而是先验地认为通过对对方车辆类型的分类+预测可以获取最优解，因此并不够准确。这个方法的另一个缺陷在于对方的策略未必是简单的极限保守和极限激进两种，因此这个预测是不准确的。这也是我们在今后实验设计中需要考虑的问题：我们的对弈方驾驶车辆是怎样进行驾驶的？应该采取怎样的策略？\par

%%% 其它部分
%\backmatter

%% 本科生要这几个索引，研究生不要。选择性留下。
% 插图索引
%\listoffigures
%% 表格索引
%\listoftables
%% 公式索引
%\listofequations


%% 参考文献
% 注意：至少需要引用一篇参考文献，否则下面两行可能引起编译错误。
% 如果不需要参考文献，请将下面两行删除或注释掉。
% 数字式引用
%\bibliographystyle{thuthesis-numeric}
% 作者-年份式引用
%\bibliographystyle{thuthesis-author-year}
\bibliographystyle{thuthesis-bachelor}
\bibliography{ref/appendix_references}


%% 致谢
%\include{data/ack}

%% 附录
%\begin{appendix}
%\input{data/appendix01}
%\end{appendix}

%% 个人简历
%\include{data/resume}

%% 本科生进行格式审查是需要下面这个表格，答辩可能不需要。选择性留下。
% 综合论文训练记录表
%\includepdf[pages=-]{scan-record.pdf}

%\end{document}
