\chapter{总结及结论}

\section{理论成果总结}
本篇论文主要设计了一个分层强化学习的算法，基于优势函数奖励的分层强化学习，HAAR（Hierarchical reinforcement learning with Advantage-based Auxiliary Reward）。其基本思路是，通过利用上层策略习得的优势函数，我们把外界环境反馈给智能体的稀疏奖励迅速反馈给下层策略，从而加速探索和学习。HAAR致力于解决分层强化学习当中的稀疏奖励问题。

HAAR算法的整体流程如下。首先，我们在极为简单的任务中预先训练一批多样化的底层策略，这已经被许多论文研究并解决。接下来，在一个具体层次化任务中，我们采用预训练的底层策略，并从头开始训练上层策略。我们利用同策略的训练方法，不断采集环境经验，通过环境给上层策略的反馈，以及基于上层策略优势函数的辅助奖励给下层策略的反馈，来联合地优化上下层策略。

分层强化学习在强化学习的研究中得到越来越多的重视。这是基于人类解决问题的一个自然地思路：一个复杂的问题往往可以分解为不同阶段的小问题。这类算法往往具有两类特性。其一，分层的结构往往使得策略与具体任务解耦，带来了下层策略的可迁移特性。因此，分层强化学习（HRL）与迁移学习（Transfer Learning）、元学习（Meta Learning）具有密不可分的关系。在这种情形下，人们又常常称之为``多任务强化学习''（Multi-task RL）。由此，HRL可以解决较为复杂的问题。另一方面，分层强化学习可以解决稀疏奖励的问题。通过对问题进行层层抽象，每一层各自的强化学习算法就只需要专注于解决本层的问题，这往往能够大大减少智能体面临的任务长度，从而提升采样效率。HAAR算法其实主要就是致力于解决这第二类问题。当然，由于其结构的分层特性，它也具备了迁移的能力，这是我们设计实验时的基本思路决定的。

为了检验我们的算法和其他HRL算法的区别，我们进行了大量的实验。在强化学习社区公认的Mujoco物理仿真环境中，我们设计了蚂蚁迷宫环境，游泳体迷宫环境，蚂蚁食物采集环境，四房间环境等等。同时，我们还设计了一个自动驾驶仿真环境来实现算法在真实场景中的迁移。在蚂蚁迷宫环境，游泳体迷宫环境和蚂蚁食物采集环境中，HAAR都取得了比学界最优算法还要优异的成绩。而在四房间环境中，HAAR并没有取得明显的优势，其原因还有待进一步探索。

本算法的贡献总结如下：
\begin{enumerate}
  \item 我们提出了一种分层强化学习的新框架HAAR，适用于各类分层问题；
  \item 我们提出了一种一般的、易于计算的辅助奖励函数；
  \item 我们从数学上证明了，只要上下层优化的函数能够分别单调地优化各自的目标函数，这个分层结构的整体目标函数就能够得到单调的优化；
  \item 我们提出了一种对下层策略执行长度模拟退火的加速训练方法；
  \item 通过限制环境的不完全可观测性，我们保证了上下层策略的可迁移特性；
  \item 通过大量的实验，我们证明了HAAR算法在分层任务中具备极大的优势。
\end{enumerate}

\section{工作内容总结}
本毕业设计主要实现了一个分层强化学习的算法，搭建了大量的实验环境，并完成了一篇论文\cite{HAAR}，已经投稿至人工智能领域的顶级会议NuerIPS 2019。同时，该项目最终作为一个校企合作项目交付。毕业设计的完成人同时也是该篇论文的第一作者。

在这些工作中，作者个人参与并主要负责了所有的工作内容，包括文献调研，算法设计，实验设计，代码实现，论文撰写等。其中，算法的证明部分由唐敏学协助完成，算法的设计部分由李斯源和作者共同完成，唐敏学和李斯源也贡献了一部分的代码和英文论文。自动驾驶第一阶段的实验部分主要由王同瀚完成。

我们的代码将会在本论文发表不久后开源在GitHub\footnote{代码详见\url{https://github.com/ArayCHN/hierarchical_RL}}，相关视频也已经在网站上公开\footnote{视频详见\url{http://bit.ly/2JxA0eN}}。

\section{未来研究方向}
本研究为未来的算法指明了一些方向，在此我们继续展望一些有待完成的研究。

限于本论文提交时间与项目不完全重合的原因，在未来研究之前，我们仍有待完成一部分的控制变量实验设计，从而更为严谨地证明HAAR算法的有效性。在自动驾驶环岛路径规划这一复杂场景也有待进一步的探索和代码实现。接下来，我们将指出后续的研究方向。

首先，HAAR算法是针对上下两层策略进行的设计。但是分层强化学习本身并不局限于两层策略，可能有三层或者更多层。假如想要拓展HAAR到更多层次，我们就不能再采用预训练的方法，而可能需要进行从零开始的训练，这需要我们的算法结构发生一些改变。

其次，HAAR算法目前一个很大的局限性就在于它要求使用同策略的训练方法（on-policy training）。和异策略方法（off-policy）相比，同策略方法在采样效率上存在很大的局限性。事实上，在算法中我们只需要优化方法大致单调增即可，并不需要它严格单调增，那么这时一些异策略算法也是可行的。将HAAR和异策略训练相结合也将是一个有趣的研究方向。

最后，我们在实验中发现HAAR训练出来的策略具有较强的可迁移性。在后续研究中，我们呼吁研究者们关注这类针对稀疏奖励问题的分层强化学习算法在迁移性上的特征。实际中多数问题可能都是稀疏的，因此可迁移的分层强化学习也会大有用武之地。将HAAR与迁移学习相结合，将会是一个可行的方向。
