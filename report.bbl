\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{#1}
\providecommand{\href}[2]{\url{#2}}
\providecommand{\doi}[1]{DOI: \href{https://doi.org/#1}{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax\relax\else
  \urlstyle{same}\fi

\bibitem[Duan\ et~al.(2016)Duan, Chen, Houthooft, Schulman, and
  Abbeel]{benchmarking_RL}
Duan Y, Chen X, Houthooft R, et~al.
\newblock Benchmarking deep reinforcement learning for continuous
  control\allowbreak[C]//\allowbreak{}Proceedings of The 33rd International
  Conference on Machine Learning.
\newblock [S.l.: s.n.], 2016: 1329-1338.

\bibitem[Sutton\ et~al.()Sutton and Barto]{Sutton_book}
Sutton R~S, Barto A~G.
\newblock Reinforcement learning: An introduction\allowbreak[M].
\newblock Second ed.
\newblock [S.l.]: The MIT Press

\bibitem[Sutton(1988)]{Sutton_problem_formulation}
Sutton R~S.
\newblock Learning to predict by the methods of temporal
  differences\allowbreak[J].
\newblock Machine Learning, 1988, 3\penalty0 (1): 9-44.

\bibitem[Bellman(1957)]{Bellman_MDP}
Bellman R.
\newblock A markovian decision process\allowbreak[J].
\newblock Indiana Univ. Math. J., 1957, 6: 679-684.

\bibitem[DRL()]{DRL_for_driving}
Deep reinforcement learning framework for autonomous driving\allowbreak[J].
\newblock Electronic Imaging.

\bibitem[Bellman(2003)]{Bellman_DP}
Bellman R~E.
\newblock Dynamic programming\allowbreak[M].
\newblock [S.l.]: Dover Publications, Inc., 2003

\bibitem[Watkins\ et~al.(1992)Watkins and Dayan]{Q_learning}
Watkins C~J~C~H, Dayan P.
\newblock Q-learning\allowbreak[J].
\newblock Machine Learning, 1992, 8\penalty0 (3): 279-292.

\bibitem[{van Seijen}(2016)]{n_step_bootstrapping}
{van Seijen} H.
\newblock {Effective Multi-step Temporal-Difference Learning for Non-Linear
  Function Approximation}\allowbreak[J].
\newblock arXiv e-prints, 2016: arXiv:1608.05151.

\bibitem[{Li}(2017)]{deepRL_overview}
{Li} Y.
\newblock {Deep Reinforcement Learning: An Overview}\allowbreak[J].
\newblock arXiv e-prints, 2017: arXiv:1701.07274.

\bibitem[Mnih\ et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{atari_2013}
Mnih V, Kavukcuoglu K, Silver D, et~al.
\newblock Playing atari with deep reinforcement
  learning\allowbreak[M]//\allowbreak{}NIPS Deep Learning Workshop.
\newblock [S.l.: s.n.], 2013

\bibitem[{Lillicrap}\ et~al.(2015){Lillicrap}, {Hunt}, {Pritzel}, {Heess},
  {Erez}, {Tassa}, {Silver}, and {Wierstra}]{DDPG}
{Lillicrap} T~P, {Hunt} J~J, {Pritzel} A, et~al.
\newblock {Continuous control with deep reinforcement learning}\allowbreak[J].
\newblock arXiv e-prints, 2015: arXiv:1509.02971.

\bibitem[Mnih\ et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{A3C}
Mnih V, Badia A~P, Mirza M, et~al.
\newblock Asynchronous methods for deep reinforcement
  learning\allowbreak[C]//\allowbreak{}Balcan M~F, Weinberger K~Q.
\newblock Proceedings of Machine Learning Research: volume~48\quad Proceedings
  of The 33rd International Conference on Machine Learning.
\newblock New York, New York, USA: PMLR, 2016: 1928-1937.

\bibitem[Schulman\ et~al.(2015)Schulman, Levine, Moritz, Jordan, and
  Abbeel]{TRPO}
Schulman J, Levine S, Moritz P, et~al.
\newblock Trust region policy optimization\allowbreak[C]//\allowbreak{}ICML.
\newblock [S.l.: s.n.], 2015.

\bibitem[{Wu}\ et~al.(2017){Wu}, {Mansimov}, {Liao}, {Grosse}, and {Ba}]{ACKTR}
{Wu} Y, {Mansimov} E, {Liao} S, et~al.
\newblock {Scalable trust-region method for deep reinforcement learning using
  Kronecker-factored approximation}\allowbreak[J].
\newblock arXiv e-prints, 2017: arXiv:1708.05144.

\bibitem[Wang\ et~al.(2017)Wang, Bapst, Heess, Mnih, Munos, Kavukcuoglu, and
  de~Freitas]{ACER}
Wang Z, Bapst V, Heess N, et~al.
\newblock Sample efficient actor-critic with experience
  replay\allowbreak[C]//\allowbreak{}ICLR.
\newblock [S.l.: s.n.], 2017.

\bibitem[Schulman\ et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{PPO}
Schulman J, Wolski F, Dhariwal P, et~al.
\newblock Proximal policy optimization algorithms\allowbreak[Z].
\newblock [S.l.: s.n.], 2017.

\bibitem[Silver\ et~al.(2018)Silver, Hubert, Schrittwieser, Antonoglou, Lai,
  Guez, Lanctot, Sifre, Kumaran, Graepel, Lillicrap, Simonyan, and
  Hassabis]{AlphaZero}
Silver D, Hubert T, Schrittwieser J, et~al.
\newblock A general reinforcement learning algorithm that masters chess, shogi,
  and go through self-play: volume 362\allowbreak[Z].
\newblock American Association for the Advancement of Science, 2018: 1140-1144.

\bibitem[Tieleman\ et~al.(2012)Tieleman and Hinton]{RMSProp}
Tieleman T, Hinton G.
\newblock Lecture 6.5 rmsprop: Divide the gradient by a running average of its
  recent magnitude.\allowbreak[J].
\newblock COURSERA: Neural Networks for Machine Learning, 2012, 4.

\bibitem[Qian(1999)]{SGD_momentum}
Qian N.
\newblock On the momentum term in gradient descent learning
  algorithms\allowbreak[J].
\newblock Neural Netw., 1999, 12\penalty0 (1): 145-151.

\bibitem[{Ruder}(2016)]{GD_overview}
{Ruder} S.
\newblock {An overview of gradient descent optimization
  algorithms}\allowbreak[J].
\newblock arXiv e-prints, 2016: arXiv:1609.04747.

\bibitem[Mnih\ et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,
  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{nature2015}
Mnih V, Kavukcuoglu K, Silver D, et~al.
\newblock Human-level control through deep reinforcement
  learning\allowbreak[J].
\newblock Nature, 2015, 518: 529 EP -.

\bibitem[Krizhevsky\ et~al.(2012)Krizhevsky, Sutskever, and E.~Hinton]{AlexNet}
Krizhevsky A, Sutskever I, E.~Hinton G.
\newblock Imagenet classification with deep convolutional neural
  networks\allowbreak[J].
\newblock Neural Information Processing Systems, 2012, 25.

\bibitem[Boyd\ et~al.(2004)Boyd and Vandenberghe]{Convex_optimization_book}
Boyd S, Vandenberghe L.
\newblock Convex optimization\allowbreak[M].
\newblock [S.l.]: {Cambridge University Press}, 2004

\bibitem[Silver\ et~al.(2014)Silver, Lever, Heess, Degris, Wierstra, and
  Riedmiller]{DPG}
Silver D, Lever G, Heess N, et~al.
\newblock Deterministic policy gradient
  algorithms\allowbreak[C]//\allowbreak{}ICML'14: Proceedings of the 31st
  International Conference on International Conference on Machine Learning -
  Volume 32.
\newblock [S.l.: s.n.], 2014.

\bibitem[Degris\ et~al.(2012)Degris, White, and
  Sutton]{off-policy_actor-critic}
Degris T, White M, Sutton R~S.
\newblock Linear off-policy actor-critic\allowbreak[C]//\allowbreak{}ICML.
\newblock [S.l.: s.n.], 2012.

\bibitem[Silver\ et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, van~den
  Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot, Dieleman,
  Grewe, Nham, Kalchbrenner, Sutskever, Lillicrap, Leach, Kavukcuoglu, Graepel,
  and Hassabis]{AlphaGo}
Silver D, Huang A, Maddison C~J, et~al.
\newblock Mastering the game of {Go} with deep neural networks and tree
  search\allowbreak[J].
\newblock Nature, 2016, 529\penalty0 (7587): 484-489.

\bibitem[Oh\ et~al.()Oh, Guo, Lee, Lewis, and
  Singh]{action-conditional_prediction}
Oh J, Guo X, Lee H, et~al.
\newblock Action-conditional video prediction using deep networks in atari
  games\allowbreak[M]//\allowbreak{}Advances in Neural Information Processing
  Systems 28.
\newblock [S.l.: s.n.]

\bibitem[Racani\`{e}re\ et~al.()Racani\`{e}re, Weber, Reichert, Buesing, Guez,
  Jimenez~Rezende, Puigdom\`{e}nech~Badia, Vinyals, Heess, Li, Pascanu,
  Battaglia, Hassabis, Silver, and Wierstra]{I2A}
Racani\`{e}re S, Weber T, Reichert D, et~al.
\newblock Imagination-augmented agents for deep reinforcement
  learning\allowbreak[M]//\allowbreak{}Advances in Neural Information
  Processing Systems 30.
\newblock [S.l.: s.n.]

\bibitem[Sutton\ et~al.(1999)Sutton, Precup, and Singh]{Sutton:1999}
Sutton R~S, Precup D, Singh S.
\newblock Between mdps and semi-mdps: A framework for temporal abstraction in
  reinforcement learning\allowbreak[J].
\newblock Artif. Intell., 1999, 112\penalty0 (1-2): 181-211.

\bibitem[Dietterich(2000)]{HRL_with_maxQ}
Dietterich T~G.
\newblock Hierarchical reinforcement learning with the maxq value function
  decomposition\allowbreak[J].
\newblock J. Artif. Int. Res., 2000, 13\penalty0 (1): 227-303.

\bibitem[Dayan\ et~al.(1993)Dayan and Hinton]{FUN}
Dayan P, Hinton G~E.
\newblock Feudal reinforcement learning\allowbreak[M]//\allowbreak{}Hanson S~J,
  Cowan J~D, Giles C~L.
\newblock Advances in Neural Information Processing Systems 5.
\newblock [S.l.]: Morgan-Kaufmann, 1993: 271-278

\bibitem[Chentanez\ et~al.(2005)Chentanez, Barto, and Singh]{Barto_HRL}
Chentanez N, Barto A~G, Singh S~P.
\newblock Intrinsically motivated reinforcement
  learning\allowbreak[M]//\allowbreak{}Saul L~K, Weiss Y, Bottou L.
\newblock Advances in Neural Information Processing Systems 17.
\newblock [S.l.]: MIT Press, 2005: 1281-1288

\bibitem[Precup\ et~al.(1998)Precup, Sutton, and Singh]{Sutton:1998_options}
Precup D, Sutton R~S, Singh S.
\newblock Theoretical results on reinforcement learning with temporally
  abstract options\allowbreak[C]//\allowbreak{}European Conference on Machine
  Learning (ECML).
\newblock [S.l.]: Springer, 1998.

\bibitem[Bacon\ et~al.(2017)Bacon, Harb, and Precup]{option-critic}
Bacon P~L, Harb J, Precup D.
\newblock The option-critic architecture\allowbreak[C]//\allowbreak{}AAAI.
\newblock [S.l.: s.n.], 2017: 1726â€“1734.

\bibitem[Kulkarni\ et~al.(2016)Kulkarni, Narasimhan, Saeedi, and
  Tenenbaum]{Tenenbaum2016NIPS}
Kulkarni T~D, Narasimhan K, Saeedi A, et~al.
\newblock Hierarchical deep reinforcement learning: Integrating temporal
  abstraction and intrinsic motivation\allowbreak[M]//\allowbreak{}Advances in
  Neural Information Processing Systems 29.
\newblock [S.l.]: Curran Associates, Inc., 2016: 3675-3683

\bibitem[Nachum\ et~al.(2018)Nachum, Gu, Lee, and Levine]{HIRO}
Nachum O, Gu S~S, Lee H, et~al.
\newblock Data-efficient hierarchical reinforcement
  learning\allowbreak[M]//\allowbreak{}Bengio S, Wallach H, Larochelle H,
  et~al.
\newblock Advances in Neural Information Processing Systems 31.
\newblock [S.l.: s.n.], 2018: 3303-3313

\bibitem[Levy\ et~al.(2018)Levy, Jr., and Saenko]{HAC}
Levy A, Jr. R~P, Saenko K.
\newblock Learning multi-level hierarchies with
  hindsight\allowbreak[C]//\allowbreak{}Proceedings of The 33rd International
  Conference on Machine Learning.
\newblock [S.l.: s.n.], 2018.

\bibitem[Vezhnevets\ et~al.(2017)Vezhnevets, Osindero, Schaul, Heess,
  Jaderberg, Silver, and Kavukcuoglu]{feudal}
Vezhnevets A~S, Osindero S, Schaul T, et~al.
\newblock Feudal networks for hierarchical reinforcement
  learning\allowbreak[C]//\allowbreak{}International Conference on Machine
  Learning.
\newblock [S.l.: s.n.], 2017: 3540-3549.

\bibitem[{Ghosh}\ et~al.(2018){Ghosh}, {Gupta}, and {Levine}]{goal-conditioned}
{Ghosh} D, {Gupta} A, {Levine} S.
\newblock {Learning Actionable Representations with Goal-Conditioned
  Policies}\allowbreak[J].
\newblock arXiv e-prints, 2018.

\bibitem[{Nachum}\ et~al.(2018){Nachum}, {Gu}, {Lee}, and
  {Levine}]{goal_repr_learning}
{Nachum} O, {Gu} S, {Lee} H, et~al.
\newblock {Near-Optimal Representation Learning for Hierarchical Reinforcement
  Learning}\allowbreak[J].
\newblock arXiv e-prints, 2018.

\bibitem[{Eysenbach}\ et~al.(2018){Eysenbach}, {Gupta}, {Ibarz}, and
  {Levine}]{DIYAN}
{Eysenbach} B, {Gupta} A, {Ibarz} J, et~al.
\newblock {Diversity is All You Need: Learning Skills without a Reward
  Function}\allowbreak[J].
\newblock arXiv e-prints, 2018.

\bibitem[Florensa\ et~al.(2017)Florensa, Duan, and Abbeel]{SNN4hrl}
Florensa C, Duan Y, Abbeel P.
\newblock Stochastic neural networks for hierarchical reinforcement
  learning\allowbreak[C]//\allowbreak{}Proceedings of The 34th International
  Conference on Machine Learning.
\newblock [S.l.: s.n.], 2017.

\bibitem[{Heess}\ et~al.(2016){Heess}, {Wayne}, {Tassa}, {Lillicrap},
  {Riedmiller}, and
  {Silver}]{Learning_and_Transfer_of_Modulated_Locomotor_Controllers}
{Heess} N, {Wayne} G, {Tassa} Y, et~al.
\newblock {Learning and Transfer of Modulated Locomotor
  Controllers}\allowbreak[J].
\newblock arXiv e-prints, 2016.

\bibitem[Frans\ et~al.(2018)Frans, Ho, Chen, Abbeel, and Schulman]{MLSH}
Frans K, Ho J, Chen X, et~al.
\newblock {META} {LEARNING} {SHARED}
  {HIERARCHIES}\allowbreak[C]//\allowbreak{}International Conference on
  Learning Representations.
\newblock [S.l.: s.n.], 2018.

\bibitem[Jang\ et~al.(2017)Jang, Gu, and Poole]{categorical_gradient}
Jang E, Gu S, Poole B.
\newblock Categorical reparameterization with
  gumbel-softmax\allowbreak[C]//\allowbreak{}International Conference on
  Learning Representations.
\newblock [S.l.: s.n.], 2017.

\bibitem[Dwiel\ et~al.(2019)Dwiel, Candadai, Phielipp, and
  Bansal]{sensitive_to_goal_space}
Dwiel Z, Candadai M, Phielipp M~J, et~al.
\newblock Hierarchical policy learning is sensitive to goal space
  design\allowbreak[Z].
\newblock [S.l.: s.n.], 2019.

\bibitem[Schulman\ et~al.(2016)Schulman, Moritz, Levine, Jordan, and
  Abbeel]{GAE}
Schulman J, Moritz P, Levine S, et~al.
\newblock High-dimensional continuous control using generalized advantage
  estimation\allowbreak[C]//\allowbreak{}Proceedings of the International
  Conference on Learning Representations (ICLR).
\newblock [S.l.: s.n.], 2016.

\bibitem[Sutton\ et~al.(2000)Sutton, McAllester, Singh, and
  Mansour]{policy_gradient_theorem}
Sutton R~S, McAllester D~A, Singh S~P, et~al.
\newblock Policy gradient methods for reinforcement learning with function
  approximation\allowbreak[M]//\allowbreak{}Solla S~A, Leen T~K, M\"{u}ller K.
\newblock Advances in Neural Information Processing Systems 12.
\newblock [S.l.]: MIT Press, 2000: 1057-1063

\bibitem[Kakade\ et~al.(2002)Kakade and Langford]{TRPO_pre}
Kakade S, Langford J.
\newblock Approximately optimal approximate reinforcement
  learning\allowbreak[C]//\allowbreak{}ICML '02: Proceedings of the Nineteenth
  International Conference on Machine Learning.
\newblock [S.l.: s.n.], 2002: 267-274.

\end{thebibliography}
