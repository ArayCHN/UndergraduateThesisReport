\begin{thebibliography}{34}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{#1}
\providecommand{\href}[2]{\url{#2}}
\providecommand{\doi}[1]{DOI: \href{https://doi.org/#1}{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax\relax\else
  \urlstyle{same}\fi

\bibitem[way()]{waymo}
Waymo\allowbreak[EB/OL].
\newblock \url{https://waymo.com}.

\bibitem[Mnih\ et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,
  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{nature2015}
Mnih V, Kavukcuoglu K, Silver D, et~al.
\newblock Human-level control through deep reinforcement
  learning\allowbreak[J].
\newblock Nature, 2015, 518: 529 EP -.

\bibitem[Kober\ et~al.(2013)Kober, Bagnell, and Peters]{RL_in_robotics}
Kober J, Bagnell J~A, Peters J.
\newblock Reinforcement learning in robotics: A survey\allowbreak[J].
\newblock Int. J. Rob. Res., 2013, 32\penalty0 (11): 1238-1274.

\bibitem[Polydoros\ et~al.(2017)Polydoros and
  Nalpantidis]{Model-Based_RL_robotics}
Polydoros A~S, Nalpantidis L.
\newblock Survey of model-based reinforcement learning: Applications on
  robotics\allowbreak[J].
\newblock Journal of Intelligent {\&} Robotic Systems, 2017, 86\penalty0 (2):
  153-173.

\bibitem[DRL()]{DRL_for_driving}
Deep reinforcement learning framework for autonomous driving\allowbreak[J].
\newblock Electronic Imaging.

\bibitem[Lopez\ et~al.(2018)Lopez, Behrisch, Bieker-Walz, Erdmann,
  Fl{\"o}tter{\"o}d, Hilbrich, L{\"u}cken, Rummel, Wagner, and
  Wie{\ss}ner]{SUMO}
Lopez P~A, Behrisch M, Bieker-Walz L, et~al.
\newblock Microscopic traffic simulation using
  sumo\allowbreak[C/OL]//\allowbreak{}The 21st IEEE International Conference on
  Intelligent Transportation Systems.
\newblock IEEE, 2018.
\newblock \url{https://elib.dlr.de/124092/}.

\bibitem[Michel(1998)]{Webots}
Michel O.
\newblock Webots: Symbiosis between virtual and real mobile
  robots\allowbreak[C]//\allowbreak{}VW '98: Proceedings of the First
  International Conference on Virtual Worlds.
\newblock [S.l.: s.n.], 1998: 254-263.

\bibitem[Sutton\ et~al.()Sutton and Barto]{Sutton_book}
Sutton R~S, Barto A~G.
\newblock Reinforcement learning: An introduction\allowbreak[M].
\newblock Second ed.
\newblock [S.l.]: The MIT Press

\bibitem[Sutton(1988)]{Sutton_problem_formulation}
Sutton R~S.
\newblock Learning to predict by the methods of temporal
  differences\allowbreak[J].
\newblock Machine Learning, 1988, 3\penalty0 (1): 9-44.

\bibitem[Bellman(1957)]{Bellman_MDP}
Bellman R.
\newblock A markovian decision process\allowbreak[J].
\newblock Indiana Univ. Math. J., 1957, 6: 679-684.

\bibitem[Bellman(2003)]{Bellman_DP}
Bellman R~E.
\newblock Dynamic programming\allowbreak[M].
\newblock [S.l.]: Dover Publications, Inc., 2003

\bibitem[Watkins\ et~al.(1992)Watkins and Dayan]{Q_learning}
Watkins C~J~C~H, Dayan P.
\newblock Q-learning\allowbreak[J].
\newblock Machine Learning, 1992, 8\penalty0 (3): 279-292.

\bibitem[{van Seijen}(2016)]{n_step_bootstrapping}
{van Seijen} H.
\newblock {Effective Multi-step Temporal-Difference Learning for Non-Linear
  Function Approximation}\allowbreak[J].
\newblock arXiv e-prints, 2016: arXiv:1608.05151.

\bibitem[{Li}(2017)]{deepRL_overview}
{Li} Y.
\newblock {Deep Reinforcement Learning: An Overview}\allowbreak[J].
\newblock arXiv e-prints, 2017: arXiv:1701.07274.

\bibitem[Mnih\ et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{atari_2013}
Mnih V, Kavukcuoglu K, Silver D, et~al.
\newblock Playing atari with deep reinforcement
  learning\allowbreak[M]//\allowbreak{}NIPS Deep Learning Workshop.
\newblock [S.l.: s.n.], 2013

\bibitem[{Lillicrap}\ et~al.(2015){Lillicrap}, {Hunt}, {Pritzel}, {Heess},
  {Erez}, {Tassa}, {Silver}, and {Wierstra}]{DDPG}
{Lillicrap} T~P, {Hunt} J~J, {Pritzel} A, et~al.
\newblock {Continuous control with deep reinforcement learning}\allowbreak[J].
\newblock arXiv e-prints, 2015: arXiv:1509.02971.

\bibitem[Mnih\ et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{A3C}
Mnih V, Badia A~P, Mirza M, et~al.
\newblock Asynchronous methods for deep reinforcement
  learning\allowbreak[C]//\allowbreak{}Balcan M~F, Weinberger K~Q.
\newblock Proceedings of Machine Learning Research: volume~48\quad Proceedings
  of The 33rd International Conference on Machine Learning.
\newblock New York, New York, USA: PMLR, 2016: 1928-1937.

\bibitem[Schulman\ et~al.(2015)Schulman, Levine, Moritz, Jordan, and
  Abbeel]{TRPO}
Schulman J, Levine S, Moritz P, et~al.
\newblock Trust region policy optimization\allowbreak[C]//\allowbreak{}ICML.
\newblock [S.l.: s.n.], 2015.

\bibitem[{Wu}\ et~al.(2017){Wu}, {Mansimov}, {Liao}, {Grosse}, and {Ba}]{ACKTR}
{Wu} Y, {Mansimov} E, {Liao} S, et~al.
\newblock {Scalable trust-region method for deep reinforcement learning using
  Kronecker-factored approximation}\allowbreak[J].
\newblock arXiv e-prints, 2017: arXiv:1708.05144.

\bibitem[Wang\ et~al.(2017)Wang, Bapst, Heess, Mnih, Munos, Kavukcuoglu, and
  de~Freitas]{ACER}
Wang Z, Bapst V, Heess N, et~al.
\newblock Sample efficient actor-critic with experience
  replay\allowbreak[C]//\allowbreak{}ICLR.
\newblock [S.l.: s.n.], 2017.

\bibitem[Schulman\ et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{PPO}
Schulman J, Wolski F, Dhariwal P, et~al.
\newblock Proximal policy optimization algorithms\allowbreak[Z].
\newblock [S.l.: s.n.], 2017.

\bibitem[Silver\ et~al.(2018)Silver, Hubert, Schrittwieser, Antonoglou, Lai,
  Guez, Lanctot, Sifre, Kumaran, Graepel, Lillicrap, Simonyan, and
  Hassabis]{AlphaZero}
Silver D, Hubert T, Schrittwieser J, et~al.
\newblock A general reinforcement learning algorithm that masters chess, shogi,
  and go through self-play: volume 362\allowbreak[Z].
\newblock American Association for the Advancement of Science, 2018: 1140-1144.

\bibitem[Tieleman\ et~al.(2012)Tieleman and Hinton]{RMSProp}
Tieleman T, Hinton G.
\newblock Lecture 6.5 rmsprop: Divide the gradient by a running average of its
  recent magnitude.\allowbreak[J].
\newblock COURSERA: Neural Networks for Machine Learning, 2012, 4.

\bibitem[Qian(1999)]{SGD_momentum}
Qian N.
\newblock On the momentum term in gradient descent learning
  algorithms\allowbreak[J].
\newblock Neural Netw., 1999, 12\penalty0 (1): 145-151.

\bibitem[{Ruder}(2016)]{GD_overview}
{Ruder} S.
\newblock {An overview of gradient descent optimization
  algorithms}\allowbreak[J].
\newblock arXiv e-prints, 2016: arXiv:1609.04747.

\bibitem[Krizhevsky\ et~al.(2012)Krizhevsky, Sutskever, and E.~Hinton]{AlexNet}
Krizhevsky A, Sutskever I, E.~Hinton G.
\newblock Imagenet classification with deep convolutional neural
  networks\allowbreak[J].
\newblock Neural Information Processing Systems, 2012, 25.

\bibitem[Boyd\ et~al.(2004)Boyd and Vandenberghe]{Convex_optimization_book}
Boyd S, Vandenberghe L.
\newblock Convex optimization\allowbreak[M].
\newblock [S.l.]: {Cambridge University Press}, 2004

\bibitem[Silver\ et~al.(2014)Silver, Lever, Heess, Degris, Wierstra, and
  Riedmiller]{DPG}
Silver D, Lever G, Heess N, et~al.
\newblock Deterministic policy gradient
  algorithms\allowbreak[C]//\allowbreak{}ICML'14: Proceedings of the 31st
  International Conference on International Conference on Machine Learning -
  Volume 32.
\newblock [S.l.: s.n.], 2014.

\bibitem[Degris\ et~al.(2012)Degris, White, and
  Sutton]{off-policy_actor-critic}
Degris T, White M, Sutton R~S.
\newblock Linear off-policy actor-critic\allowbreak[C]//\allowbreak{}ICML.
\newblock [S.l.: s.n.], 2012.

\bibitem[Silver\ et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, van~den
  Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot, Dieleman,
  Grewe, Nham, Kalchbrenner, Sutskever, Lillicrap, Leach, Kavukcuoglu, Graepel,
  and Hassabis]{AlphaGo}
Silver D, Huang A, Maddison C~J, et~al.
\newblock Mastering the game of {Go} with deep neural networks and tree
  search\allowbreak[J].
\newblock Nature, 2016, 529\penalty0 (7587): 484-489.

\bibitem[Oh\ et~al.()Oh, Guo, Lee, Lewis, and
  Singh]{action-conditional_prediction}
Oh J, Guo X, Lee H, et~al.
\newblock Action-conditional video prediction using deep networks in atari
  games\allowbreak[M]//\allowbreak{}Advances in Neural Information Processing
  Systems 28.
\newblock [S.l.: s.n.]

\bibitem[Racani\`{e}re\ et~al.()Racani\`{e}re, Weber, Reichert, Buesing, Guez,
  Jimenez~Rezende, Puigdom\`{e}nech~Badia, Vinyals, Heess, Li, Pascanu,
  Battaglia, Hassabis, Silver, and Wierstra]{I2A}
Racani\`{e}re S, Weber T, Reichert D, et~al.
\newblock Imagination-augmented agents for deep reinforcement
  learning\allowbreak[M]//\allowbreak{}Advances in Neural Information
  Processing Systems 30.
\newblock [S.l.: s.n.]

\bibitem[Dhariwal\ et~al.(2017)Dhariwal, Hesse, Klimov, Nichol, Plappert,
  Radford, Schulman, Sidor, Wu, and Zhokhov]{openAI_baselines}
Dhariwal P, Hesse C, Klimov O, et~al.
\newblock Openai baselines\allowbreak[J].
\newblock GitHub repository, 2017.

\bibitem[Hausknecht\ et~al.(2015)Hausknecht and Stone]{DRQ}
Hausknecht M~J, Stone P.
\newblock Deep recurrent q-learning for partially observable
  mdps\allowbreak[C]//\allowbreak{}AAAI Fall Symposia.
\newblock [S.l.: s.n.], 2015.

\end{thebibliography}
