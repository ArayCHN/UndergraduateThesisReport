\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/R>>}
\providecommand {\FN@pp@footnotehinttrue }{}
\providecommand {\FN@pp@footnote@aux }[2]{}
\FN@pp@footnotehinttrue 
\FN@pp@footnotehinttrue 
\FN@pp@footnotehinttrue 
\FN@pp@footnotehinttrue 
\FN@pp@footnotehinttrue 
\FN@pp@footnotehinttrue 
\FN@pp@footnotehinttrue 
\FN@pp@footnotehinttrue 
\FN@pp@footnotehinttrue 
\citation{waymo}
\citation{nature2015}
\citation{RL_in_robotics}
\citation{Model-Based_RL_robotics}
\citation{DRL_for_driving}
\citation{DRL_for_driving}
\HyPL@Entry{3<</S/D>>}
\@writefile{toc}{\contentsline {section}{\numberline {1}研究课题介绍}{1}{section.0.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}选题背景及意义}{1}{subsection.0.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}选题背景}{1}{subsubsection.0.1.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}自动驾驶的问题}{1}{subsubsection.0.1.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces 自动驾驶的基本框架：三个回路，从左到右依次是感知，决策，和控制\relax }}{1}{figure.caption.6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:self-driving_overview}{{1}{1}{自动驾驶的基本框架：三个回路，从左到右依次是感知，决策，和控制\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.3}强化学习的问题}{2}{subsubsection.0.1.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}场景描述}{2}{subsection.0.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces 四个实验场景，(A) 三进口，1辆/20s (B) 四进口，1辆/20s (C) 三进口，1辆/5s (D) 四进口，1辆/5s\relax }}{2}{figure.caption.7}}
\newlabel{fig:experiments}{{2}{2}{四个实验场景，(A) 三进口，1辆/20s (B) 四进口，1辆/20s (C) 三进口，1辆/5s (D) 四进口，1辆/5s\relax }{figure.caption.7}{}}
\citation{SUMO}
\citation{Webots}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}研究方案}{3}{subsection.0.1.3}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces 实验设计\relax }}{3}{table.caption.8}}
\newlabel{table:experiments}{{1}{3}{实验设计\relax }{table.caption.8}{}}
\citation{Sutton_book}
\citation{Sutton_problem_formulation}
\@writefile{toc}{\contentsline {section}{\numberline {2}理论基础}{4}{section.0.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}强化学习文献综述}{4}{subsection.0.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}问题描述}{4}{subsubsection.0.2.1.1}}
\citation{Bellman_MDP}
\citation{DRL_for_driving}
\citation{Bellman_DP}
\citation{Sutton_book}
\@writefile{loe}{\contentsline {equation}{\numberline {1}}{5}{equation.0.2.1}}
\@writefile{loe}{\contentsline {equation}{\numberline {2}}{5}{equation.0.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}典型算法综述}{5}{subsubsection.0.2.1.2}}
\citation{Bellman_DP}
\citation{Sutton_book}
\citation{Q_learning}
\citation{Sutton_problem_formulation}
\citation{n_step_bootstrapping}
\citation{Sutton_book}
\citation{deepRL_overview}
\citation{Sutton_book}
\citation{atari_2013}
\citation{deepRL_overview}
\citation{Sutton_book}
\@writefile{loe}{\contentsline {equation}{\numberline {3}}{6}{equation.0.2.3}}
\newlabel{eq:Q_learning}{{3}{6}{典型算法综述}{equation.0.2.3}{}}
\citation{DDPG}
\citation{A3C}
\citation{TRPO}
\citation{ACKTR}
\citation{ACER}
\citation{PPO}
\citation{AlphaZero}
\citation{RMSProp}
\citation{SGD_momentum}
\citation{Sutton_book}
\citation{GD_overview}
\citation{atari_2013}
\citation{nature2015}
\citation{deepRL_overview}
\citation{AlexNet}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}无模型的深度强化学习}{7}{subsubsection.0.2.1.3}}
\citation{A3C}
\citation{Sutton_book}
\citation{TRPO}
\@writefile{loe}{\contentsline {equation}{\numberline {4}}{8}{equation.0.2.4}}
\newlabel{eq:A3C_gradient1}{{4}{8}{无模型的深度强化学习}{equation.0.2.4}{}}
\@writefile{loe}{\contentsline {equation}{\numberline {5}}{8}{equation.0.2.5}}
\newlabel{eq:A3C_gradient2}{{5}{8}{无模型的深度强化学习}{equation.0.2.5}{}}
\citation{Convex_optimization_book}
\citation{ACKTR}
\citation{PPO}
\citation{DPG}
\citation{Sutton_book}
\citation{A3C}
\citation{off-policy_actor-critic}
\citation{Sutton_book}
\@writefile{loe}{\contentsline {equation}{\numberline {6}}{9}{equation.0.2.6}}
\citation{DDPG}
\citation{AlphaGo}
\citation{AlphaZero}
\@writefile{loe}{\contentsline {equation}{\numberline {7}}{10}{equation.0.2.7}}
\newlabel{eq:DPG_update}{{7}{10}{无模型的深度强化学习}{equation.0.2.7}{}}
\newlabel{eq:target_network}{{2.1.3}{10}{无模型的深度强化学习}{equation.0.2.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4}有模型的深度强化学习}{10}{subsubsection.0.2.1.4}}
\citation{Sutton_book}
\citation{Sutton_book}
\citation{action-conditional_prediction}
\citation{action-conditional_prediction}
\citation{I2A}
\@writefile{loe}{\contentsline {equation}{\numberline {8}}{11}{equation.0.2.8}}
\citation{I2A}
\citation{DRQ}
\citation{LSTM}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces I2A的基本结构：a) 一个预先训练好的神经网络，可以预测在策略$\mathaccentV {hat}05E{\pi }$下，状态$s$的一系列时序变化；b) 一个子网络，用于``理解''a)的输出结果；c) 整体架构\relax }}{12}{figure.caption.9}}
\newlabel{fig:I2A}{{3}{12}{I2A的基本结构：a) 一个预先训练好的神经网络，可以预测在策略$\hat {\pi }$下，状态$s$的一系列时序变化；b) 一个子网络，用于``理解''a)的输出结果；c) 整体架构\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}部分可观测的马尔可夫决策过程}{12}{subsection.0.2.2}}
\citation{RNN_predict_driver_intention}
\citation{explicit_decision_tree}
\citation{game_theory}
\citation{game_theory}
\citation{game_theory}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}有模型和无模型结合的高效强化学习}{13}{subsection.0.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}多任务分层强化学习}{13}{subsection.0.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}强化学习在自动驾驶领域的应用}{13}{subsection.0.2.5}}
\citation{openAI_baselines}
\citation{DDPG}
\citation{DRQ}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces 论文\cite  {game_theory}提出的基于博弈论的决策方案整体框架。\relax }}{14}{figure.caption.10}}
\newlabel{fig:game_theory_structure}{{4}{14}{论文\cite {game_theory}提出的基于博弈论的决策方案整体框架。\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}课题内容及开展情况}{14}{section.0.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}第一阶段}{14}{subsection.0.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}综述}{14}{subsubsection.0.3.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}建模方法}{15}{subsubsection.0.3.1.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}奖励函数设计}{15}{subsubsection.0.3.1.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}实验结果}{15}{subsubsection.0.3.1.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces DDPG算法训练的实验结果。（a）为平均获得的奖励值（b）为汽车在仿真环境中的平均速度（c）为平均成功率。每个数据点均为五十次训练的平均值，横坐标为百次训练，每次训练以发生事故或通过环岛为终止条件。\relax }}{15}{figure.caption.11}}
\newlabel{fig:experiment_phaseI}{{5}{15}{DDPG算法训练的实验结果。（a）为平均获得的奖励值（b）为汽车在仿真环境中的平均速度（c）为平均成功率。每个数据点均为五十次训练的平均值，横坐标为百次训练，每次训练以发生事故或通过环岛为终止条件。\relax }{figure.caption.11}{}}
\citation{I2A}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces 实验成功率统计（百分比）\relax }}{16}{table.caption.12}}
\newlabel{table:experiment_phaseI}{{2}{16}{实验成功率统计（百分比）\relax }{table.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}第二阶段}{16}{subsection.0.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4}进度计划}{16}{section.0.4}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces 进度计划\relax }}{16}{table.caption.13}}
\newlabel{table:plan}{{3}{16}{进度计划\relax }{table.caption.13}{}}
\FN@pp@footnotehinttrue 
\bibstyle{thuthesis-numeric}
\bibdata{../../references/bibtex/all_references}
\bibcite{waymo}{{1}{}{{way}}{{}}}
\bibcite{nature2015}{{2}{2015}{{Mnih\ et~al.}}{{Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik, Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis}}}
\bibcite{RL_in_robotics}{{3}{2013}{{Kober\ et~al.}}{{Kober, Bagnell, and Peters}}}
\bibcite{Model-Based_RL_robotics}{{4}{2017}{{Polydoros\ et~al.}}{{Polydoros and Nalpantidis}}}
\bibcite{DRL_for_driving}{{5}{}{{DRL}}{{}}}
\bibcite{SUMO}{{6}{2018}{{Lopez\ et~al.}}{{Lopez, Behrisch, Bieker-Walz, Erdmann, Fl{\"o}tter{\"o}d, Hilbrich, L{\"u}cken, Rummel, Wagner, and Wie{\ss }ner}}}
\bibcite{Webots}{{7}{1998}{{Michel}}{{}}}
\bibcite{Sutton_book}{{8}{}{{Sutton\ et~al.}}{{Sutton and Barto}}}
\bibcite{Sutton_problem_formulation}{{9}{1988}{{Sutton}}{{}}}
\bibcite{Bellman_MDP}{{10}{1957}{{Bellman}}{{}}}
\bibcite{Bellman_DP}{{11}{2003}{{Bellman}}{{}}}
\bibcite{Q_learning}{{12}{1992}{{Watkins\ et~al.}}{{Watkins and Dayan}}}
\bibcite{n_step_bootstrapping}{{13}{2016}{{van Seijen}}{{}}}
\bibcite{deepRL_overview}{{14}{2017}{{Li}}{{}}}
\bibcite{atari_2013}{{15}{2013}{{Mnih\ et~al.}}{{Mnih, Kavukcuoglu, Silver, Graves, Antonoglou, Wierstra, and Riedmiller}}}
\bibcite{DDPG}{{16}{2015}{{{Lillicrap}\ et~al.}}{{{Lillicrap}, {Hunt}, {Pritzel}, {Heess}, {Erez}, {Tassa}, {Silver}, and {Wierstra}}}}
\bibcite{A3C}{{17}{2016}{{Mnih\ et~al.}}{{Mnih, Badia, Mirza, Graves, Lillicrap, Harley, Silver, and Kavukcuoglu}}}
\bibcite{TRPO}{{18}{2015}{{Schulman\ et~al.}}{{Schulman, Levine, Moritz, Jordan, and Abbeel}}}
\FN@pp@footnotehinttrue 
\@writefile{toc}{\contentsline {chapter}{参考文献}{17}{section*.14}}
\FN@pp@footnotehinttrue 
\bibcite{ACKTR}{{19}{2017}{{{Wu}\ et~al.}}{{{Wu}, {Mansimov}, {Liao}, {Grosse}, and {Ba}}}}
\bibcite{ACER}{{20}{2017}{{Wang\ et~al.}}{{Wang, Bapst, Heess, Mnih, Munos, Kavukcuoglu, and de~Freitas}}}
\bibcite{PPO}{{21}{2017}{{Schulman\ et~al.}}{{Schulman, Wolski, Dhariwal, Radford, and Klimov}}}
\bibcite{AlphaZero}{{22}{2018}{{Silver\ et~al.}}{{Silver, Hubert, Schrittwieser, Antonoglou, Lai, Guez, Lanctot, Sifre, Kumaran, Graepel, Lillicrap, Simonyan, and Hassabis}}}
\bibcite{RMSProp}{{23}{2012}{{Tieleman\ et~al.}}{{Tieleman and Hinton}}}
\bibcite{SGD_momentum}{{24}{1999}{{Qian}}{{}}}
\bibcite{GD_overview}{{25}{2016}{{Ruder}}{{}}}
\bibcite{AlexNet}{{26}{2012}{{Krizhevsky\ et~al.}}{{Krizhevsky, Sutskever, and E.~Hinton}}}
\bibcite{Convex_optimization_book}{{27}{2004}{{Boyd\ et~al.}}{{Boyd and Vandenberghe}}}
\bibcite{DPG}{{28}{2014}{{Silver\ et~al.}}{{Silver, Lever, Heess, Degris, Wierstra, and Riedmiller}}}
\bibcite{off-policy_actor-critic}{{29}{2012}{{Degris\ et~al.}}{{Degris, White, and Sutton}}}
\bibcite{AlphaGo}{{30}{2016}{{Silver\ et~al.}}{{Silver, Huang, Maddison, Guez, Sifre, van~den Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot, Dieleman, Grewe, Nham, Kalchbrenner, Sutskever, Lillicrap, Leach, Kavukcuoglu, Graepel, and Hassabis}}}
\bibcite{action-conditional_prediction}{{31}{}{{Oh\ et~al.}}{{Oh, Guo, Lee, Lewis, and Singh}}}
\bibcite{I2A}{{32}{}{{Racani\`{e}re\ et~al.}}{{Racani\`{e}re, Weber, Reichert, Buesing, Guez, Jimenez~Rezende, Puigdom\`{e}nech~Badia, Vinyals, Heess, Li, Pascanu, Battaglia, Hassabis, Silver, and Wierstra}}}
\bibcite{DRQ}{{33}{2015}{{Hausknecht\ et~al.}}{{Hausknecht and Stone}}}
\bibcite{LSTM}{{34}{1997}{{Hochreiter\ et~al.}}{{Hochreiter and Schmidhuber}}}
\bibcite{RNN_predict_driver_intention}{{35}{2018}{{Zyner\ et~al.}}{{Zyner, Worrall, and Nebot}}}
\bibcite{explicit_decision_tree}{{36}{2017}{{N\ et~al.}}{{N, H, I, and A.}}}
\bibcite{game_theory}{{37}{2018}{{Tian\ et~al.}}{{Tian, Li, Li, Kolmanovsky, Girard, and Yildiz}}}
\bibcite{openAI_baselines}{{38}{2017}{{Dhariwal\ et~al.}}{{Dhariwal, Hesse, Klimov, Nichol, Plappert, Radford, Schulman, Sidor, Wu, and Zhokhov}}}
\FN@pp@footnotehinttrue 
