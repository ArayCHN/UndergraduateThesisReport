
@book{Sutton_book,
  author = {Sutton, Richard S. and Barto, Andrew G.},
  edition = {Second},
  publisher = {The MIT Press},
  title = {Reinforcement Learning: An Introduction},
  % url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018}
}

@Article{Sutton_problem_formulation,
author="Sutton, Richard S.",
title="Learning to predict by the methods of temporal differences",
journal="Machine Learning",
year="1988",
month="Aug",
day="01",
volume="3",
number="1",
pages="9--44",
issn="1573-0565",
% doi="10.1007/BF00115009",
% url="https://doi.org/10.1007/BF00115009"
}

@ARTICLE{Bellman_MDP,
    author = "Richard Bellman",
     title = "A Markovian Decision Process",
   journal = "Indiana Univ. Math. J.",
  fjournal = "Indiana University Mathematics Journal",
    volume = 6,
      year = 1957,
     issue = 4,
     pages = "679--684",
      issn = "0022-2518",
   %   coden = "IUMJAB",
   % mrclass = "",
}

@book{Bellman_DP,
 author = {Bellman, Richard Ernest},
 title = {Dynamic Programming},
 year = {2003},
 isbn = {0486428095},
 publisher = {Dover Publications, Inc.},
 % address = {New York, NY, USA},
}

@ARTICLE{n_step_bootstrapping,
       author = {{van Seijen}, Harm},
        title = "{Effective Multi-step Temporal-Difference Learning for Non-Linear Function Approximation}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Artificial Intelligence},
         year = 2016,
        month = Aug,
          eid = {arXiv:1608.05151},
        pages = {arXiv:1608.05151},
archivePrefix = {arXiv},
       eprint = {1608.05151},
 primaryClass = {cs.AI},
      %  adsurl = {https://ui.adsabs.harvard.edu/\#abs/2016arXiv160805151V},
      % adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article {DRL_for_driving,
title = "Deep Reinforcement Learning framework for Autonomous Driving",
journal = "Electronic Imaging",
% parent_itemid = "infobike://ist/ei",
% publishercode ="ist",
year = "2017",
volume = "2017",
number = "19",
publication date ="2017-01-29T00:00:00",
pages = "70-76",
itemtype = "ARTICLE",
% issn = "2470-1173",
% eissn = "2470-1173",
% url = "https://www.ingentaconnect.com/content/ist/ei/2017/00002017/00000019/art00012",
% doi = "doi:10.2352/ISSN.2470-1173.2017.19.AVM-023",
% keyword = "REINFORCEMENT LEARNING, DEEP LEARNING, AUTONOMOUS DRIVING",
author = "Sallab, Ahmad EL and Abdou, Mohammed and Perot, Etienne and Yogamani, Senthil",
}

@ARTICLE{deepRL_overview,
       author = {{Li}, Yuxi},
        title = "{Deep Reinforcement Learning: An Overview}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning},
         year = 2017,
        month = Jan,
          eid = {arXiv:1701.07274},
        pages = {arXiv:1701.07274},
archivePrefix = {arXiv},
       eprint = {1701.07274},
 primaryClass = {cs.LG},
      %  adsurl = {https://ui.adsabs.harvard.edu/\#abs/2017arXiv170107274L},
      % adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@incollection{atari_2013,
  title = {Playing Atari With Deep Reinforcement Learning},
  author = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
  booktitle = {NIPS Deep Learning Workshop},
  year = {2013}
}

@misc{openAI_baselines,
  author = {Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai and Zhokhov, Peter},
  title = {OpenAI Baselines},
  year = {2017},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openai/baselines}},
}

@inproceedings{TRPO,
  title={Trust Region Policy Optimization},
  author={John Schulman and Sergey Levine and Philipp Moritz and Michael I. Jordan and Pieter Abbeel},
  booktitle={ICML},
  year={2015}
}

@ARTICLE{DDPG,
       author = {{Lillicrap}, Timothy P. and {Hunt}, Jonathan J. and {Pritzel}, Alexander
        and {Heess}, Nicolas and {Erez}, Tom and {Tassa}, Yuval and
        {Silver}, David and {Wierstra}, Daan},
        title = "{Continuous control with deep reinforcement learning}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
         year = 2015,
        month = Sep,
          eid = {arXiv:1509.02971},
        pages = {arXiv:1509.02971},
archivePrefix = {arXiv},
       eprint = {1509.02971},
 primaryClass = {cs.LG},
      %  adsurl = {https://ui.adsabs.harvard.edu/\#abs/2015arXiv150902971L},
      % adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{ACKTR,
       author = {{Wu}, Yuhuai and {Mansimov}, Elman and {Liao}, Shun and {Grosse}, Roger
        and {Ba}, Jimmy},
        title = "{Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning},
         year = 2017,
        month = Aug,
          eid = {arXiv:1708.05144},
        pages = {arXiv:1708.05144},
archivePrefix = {arXiv},
       eprint = {1708.05144},
 primaryClass = {cs.LG},
      %  adsurl = {https://ui.adsabs.harvard.edu/\#abs/2017arXiv170805144W},
      % adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@misc{PPO,
    title={Proximal Policy Optimization Algorithms},
    author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
    year={2017},
    eprint={1707.06347},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{ACER,
  title={Sample Efficient Actor-Critic with Experience Replay},
  author={Ziyu Wang and Victor Bapst and Nicolas Heess and Volodymyr Mnih and Remi Munos and Koray Kavukcuoglu and Nando de Freitas},
  booktitle={ICLR},
  year={2017}
}

@Article{Q_learning,
author="Watkins, Christopher J. C. H.
and Dayan, Peter",
title="Q-learning",
journal="Machine Learning",
year="1992",
month="May",
day="01",
volume="8",
number="3",
pages="279--292",
% issn="1573-0565",
% doi="10.1007/BF00992698",
% url="https://doi.org/10.1007/BF00992698"
}

@Article{nature2015,
author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
title={Human-level control through deep reinforcement learning},
journal={Nature},
year={2015},
month={Feb},
day={25},
publisher={Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved. SN  -},
volume={518},
pages={529 EP  -},
% url={https://doi.org/10.1038/nature14236}
}

@article{AlexNet,
author = {Krizhevsky, Alex and Sutskever, Ilya and E. Hinton, Geoffrey},
year = {2012},
month = {01},
pages = {},
title = {ImageNet Classification with Deep Convolutional Neural Networks},
volume = {25},
journal = {Neural Information Processing Systems},
% doi = {10.1145/3065386}
}

@InProceedings{A3C,
  title =    {Asynchronous Methods for Deep Reinforcement Learning},
  author =   {Volodymyr Mnih and Adria Puigdomenech Badia and Mehdi Mirza and Alex Graves and Timothy Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
  booktitle =    {Proceedings of The 33rd International Conference on Machine Learning},
  pages =    {1928--1937},
  year =   {2016},
  editor =   {Maria Florina Balcan and Kilian Q. Weinberger},
  volume =   {48},
  series =   {Proceedings of Machine Learning Research},
  address =    {New York, New York, USA},
  month =    {20--22 Jun},
  publisher =    {PMLR},
  % pdf =    {http://proceedings.mlr.press/v48/mniha16.pdf},
  % url =    {http://proceedings.mlr.press/v48/mniha16.html},
}

@article{RMSProp,
    title={Lecture 6.5 rmsprop: Divide the gradient by a running average of its recent magnitude.},
    author={Tieleman, Tijmen and Hinton, Geoffrey},
    year={2012},
    volume = {4},
    journal = {COURSERA: Neural Networks for Machine Learning},
}

@ARTICLE{GD_overview,
       author = {{Ruder}, Sebastian},
        title = "{An overview of gradient descent optimization algorithms}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning},
         year = 2016,
        month = Sep,
          eid = {arXiv:1609.04747},
        pages = {arXiv:1609.04747},
archivePrefix = {arXiv},
       eprint = {1609.04747},
 primaryClass = {cs.LG},
      %  adsurl = {https://ui.adsabs.harvard.edu/\#abs/2016arXiv160904747R},
      % adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{SGD_momentum,
 author = {Qian, Ning},
 title = {On the Momentum Term in Gradient Descent Learning Algorithms},
 journal = {Neural Netw.},
 issue_date = {Jan. 1999},
 volume = {12},
 number = {1},
 month = jan,
 year = {1999},
 issn = {0893-6080},
 pages = {145--151},
 numpages = {7},
 % url = {http://dx.doi.org/10.1016/S0893-6080(98)00116-6},
 % doi = {10.1016/S0893-6080(98)00116-6},
 acmid = {307376},
 publisher = {Elsevier Science Ltd.},
 % address = {Oxford, UK, UK},
} 

@inproceedings{DPG,
 author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
 title = {Deterministic Policy Gradient Algorithms},
 booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
 series = {ICML'14},
 year = {2014},
 % location = {Beijing, China},
 pages = {I-387--I-395},
 % url = {http://dl.acm.org/citation.cfm?id=3044805.3044850},
 % acmid = {3044850},
 publisher = {JMLR.org},
}

@inproceedings{off-policy_actor-critic,
  title={Linear Off-Policy Actor-Critic},
  author={Thomas Degris and Martha White and Richard S. Sutton},
  booktitle={ICML},
  year={2012}
}

@article{RL_in_robotics_survey,
 author = {Kober, Jens and Bagnell, J. Andrew and Peters, Jan},
 title = {Reinforcement Learning in Robotics: A Survey},
 journal = {Int. J. Rob. Res.},
 issue_date = {September 2013},
 volume = {32},
 number = {11},
 month = sep,
 year = {2013},
 issn = {0278-3649},
 pages = {1238--1274},
 numpages = {37},
 % url = {http://dx.doi.org/10.1177/0278364913495721},
 % doi = {10.1177/0278364913495721},
 acmid = {2528334},
 publisher = {Sage Publications, Inc.},
 % address = {Thousand Oaks, CA, USA},
 % keywords = {Reinforcement learning, learning control, robot, survey},
} 

@article{AlphaGo,
  added-at = {2016-03-11T14:36:05.000+0100},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  doi = {10.1038/nature16961},
  journal = {Nature},
  month = jan,
  number = 7587,
  pages = {484--489},
  publisher = {Nature Publishing Group},
  title = {Mastering the Game of {Go} with Deep Neural Networks and Tree Search},
  volume = 529,
  year = 2016
}

@article {AlphaZero,
    author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
    title = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
    volume = {362},
    number = {6419},
    pages = {1140--1144},
    year = {2018},
    doi = {10.1126/science.aar6404},
    publisher = {American Association for the Advancement of Science},
    % issn = {0036-8075},
    % URL = {http://science.sciencemag.org/content/362/6419/1140},
    % eprint = {http://science.sciencemag.org/content/362/6419/1140.full.pdf},
    journal = {Science}
}

@book{Convex_optimization_book,
  author = {Boyd, Stephen and Vandenberghe, Lieven},
  howpublished = {Hardcover},
  isbn = {0521833787},
  month = {March},
  priority = {4},
  publisher = {{Cambridge University Press}},
  timestamp = {2006-04-12T21:13:14.000+0200},
  title = {Convex Optimization},
  year = 2004
}

@incollection{I2A,
title = {Imagination-Augmented Agents for Deep Reinforcement Learning},
author = {Racani\`{e}re, S\'{e}bastien and Weber, Theophane and Reichert, David and Buesing, Lars and Guez, Arthur and Jimenez Rezende, Danilo and Puigdom\`{e}nech Badia, Adri\`{a} and Vinyals, Oriol and Heess, Nicolas and Li, Yujia and Pascanu, Razvan and Battaglia, Peter and Hassabis, Demis and Silver, David and Wierstra, Daan},
booktitle = {Advances in Neural Information Processing Systems 30},
% editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5690--5701},
year = {2017},
publisher = {Curran Associates, Inc.},
% url = {http://papers.nips.cc/paper/7152-imagination-augmented-agents-for-deep-reinforcement-learning.pdf}
}

@incollection{action-conditional_prediction,
title = {Action-Conditional Video Prediction using Deep Networks in Atari Games},
author = {Oh, Junhyuk and Guo, Xiaoxiao and Lee, Honglak and Lewis, Richard L and Singh, Satinder},
booktitle = {Advances in Neural Information Processing Systems 28},
% editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {2863--2871},
year = {2015},
publisher = {Curran Associates, Inc.},
% url = {http://papers.nips.cc/paper/5859-action-conditional-video-prediction-using-deep-networks-in-atari-games.pdf}
}

@Article{Model-Based_RL_robotics,
author="Polydoros, Athanasios S.
and Nalpantidis, Lazaros",
title="Survey of Model-Based Reinforcement Learning: Applications on Robotics",
journal="Journal of Intelligent {\&} Robotic Systems",
year="2017",
month="May",
day="01",
volume="86",
number="2",
pages="153--173",
issn="1573-0409",
% doi="10.1007/s10846-017-0468-y",
% url="https://doi.org/10.1007/s10846-017-0468-y"
}

@article{RL_in_robotics,
 author = {Kober, Jens and Bagnell, J. Andrew and Peters, Jan},
 title = {Reinforcement Learning in Robotics: A Survey},
 journal = {Int. J. Rob. Res.},
 issue_date = {September 2013},
 volume = {32},
 number = {11},
 month = sep,
 year = {2013},
 issn = {0278-3649},
 pages = {1238--1274},
 numpages = {37},
} 


@misc{waymo,
  title = {Waymo},
  url="https://waymo.com"
}

 @inproceedings{SUMO,
          title = {Microscopic Traffic Simulation using SUMO},
         author = {Pablo Alvarez Lopez and Michael Behrisch and Laura Bieker-Walz and Jakob Erdmann and Yun-Pang Fl{\"o}tter{\"o}d and Robert Hilbrich and Leonhard L{\"u}cken and Johannes Rummel and Peter Wagner and Evamarie Wie{\ss}ner},
      publisher = {IEEE},
      booktitle = {The 21st IEEE International Conference on Intelligent Transportation Systems},
           year = {2018},
        journal = {IEEE Intelligent Transportation Systems Conference (ITSC)},
 }

 @inproceedings{Webots,
 author = {Michel, Olivier},
 title = {Webots: Symbiosis Between Virtual and Real Mobile Robots},
 booktitle = {Proceedings of the First International Conference on Virtual Worlds},
 series = {VW '98},
 year = {1998},
 isbn = {3-540-64780-5},
 pages = {254--263},
 numpages = {10}
} 

@inproceedings{DRQ,
  title={Deep Recurrent Q-Learning for Partially Observable MDPs},
  author={Matthew J. Hausknecht and Peter Stone},
  booktitle={AAAI Fall Symposia},
  year={2015}
}

@article{LSTM,
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal = {Neural computation},
  number = 8,
  pages = {1735--1780},
  publisher = {MIT Press},
  title = {Long short-term memory},
  volume = 9,
  year = 1997
}

@misc{game_theory,
    title={Adaptive Game-Theoretic Decision Making for Autonomous Vehicle Control at Roundabouts},
    author={Ran Tian and Sisi Li and Nan Li and Ilya Kolmanovsky and Anouck Girard and Yildiray Yildiz},
    year={2018},
    eprint={1810.00829},
    archivePrefix={arXiv},
    primaryClass={cs.GT}
}

@article{RNN_predict_driver_intention,
author = {Zyner, Alex and Worrall, Stewart and Nebot, Eduardo},
year = {2018},
month = {02},
pages = {1-1},
title = {A Recurrent Neural Network Solution for Predicting Driver Intention at Unsignalized Intersections},
volume = {PP},
journal = {IEEE Robotics and Automation Letters},
}

@article{explicit_decision_tree,
author = {Li N and Chen H and Kolmanovsky I and Girard A.},
year = {2017},
title = {An Explicit Decision Tree Approach for Automated Driving},
volume = {1},
journal = {ASME. Dynamic Systems and Control Conference},
}

@article{TDM,
author = {Pong, Vitchyr and Gu, Shixiang and Dalal, Murtaza and Levine, Sergey},
year = {2018},
month = {02},
pages = {},
booktitle={ICLR},
title = {Temporal Difference Models: Model-Free Deep RL for Model-Based Control}
}

@InProceedings{MBMF_trajectory-centric,
  title =    {Combining Model-Based and Model-Free Updates for Trajectory-Centric Reinforcement Learning},
  author =   {Yevgen Chebotar and Karol Hausman and Marvin Zhang and Gaurav Sukhatme and Stefan Schaal and Sergey Levine},
  booktitle =    {Proceedings of the 34th International Conference on Machine Learning},
  pages =    {703--711},
  year =     {2017},
  volume =   {70},
  series =   {Proceedings of Machine Learning Research},
  month =    {06--11 Aug},
}

@misc{MBMF,
    title={MBMF: Model-Based Priors for Model-Free Reinforcement Learning},
    author={Somil Bansal and Roberto Calandra and Kurtland Chua and Sergey Levine and Claire Tomlin},
    year={2017},
    eprint={1709.03153},
    archivePrefix={arXiv},
}

@incollection{differentiable_MPC,
title = {Differentiable MPC for End-to-end Planning and Control},
author = {Amos, Brandon and Jimenez, Ivan and Sacks, Jacob and Boots, Byron and Kolter, J. Zico},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {8299--8310},
year = {2018},
publisher = {Curran Associates, Inc.},
}

@ARTICLE{MPC_VS_RL,
author={D. Ernst and M. Glavic and F. Capitanescu and L. Wehenkel}, 
journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)}, 
title={Reinforcement Learning Versus Model Predictive Control: A Comparison on a Power System Problem}, 
year={2009}, 
volume={39}, 
number={2}, 
pages={517-529},
month={April},
}

@article{MPC,
title = "Model predictive control: past, present and future",
journal = "Computers \& Chemical Engineering",
volume = "23",
number = "4",
pages = "667 - 682",
year = "1999",
author = "Manfred Morari and Jay H. Lee",
}

@ARTICLE{Gaussian_process, 
author={M. P. Deisenroth and D. Fox and C. E. Rasmussen}, 
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
title={Gaussian Processes for Data-Efficient Learning in Robotics and Control}, 
year={2015}, 
volume={37}, 
number={2}, 
pages={408-423}, 
month={Feb},
}

@incollection{Bayesian_optimization,
title = {Practical Bayesian Optimization of Machine Learning Algorithms},
author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
booktitle = {Advances in Neural Information Processing Systems 25},
%editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {2951--2959},
year = {2012},
publisher = {Curran Associates, Inc.},
}

@incollection{NIPS2015_5796,
title = {Learning Continuous Control Policies by Stochastic Value Gradients},
author = {Heess, Nicolas and Wayne, Gregory and Silver, David and Lillicrap, Timothy and Erez, Tom and Tassa, Yuval},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {2944--2952},
year = {2015},
publisher = {Curran Associates, Inc.},
%url = {http://papers.nips.cc/paper/5796-learning-continuous-control-policies-by-stochastic-value-gradients.pdf}
}
