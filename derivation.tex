\documentclass[degree=bachelor, tocarialchapter, pifootnote]{thuthesis}
% 选项
%   degree=[bachelor|master|doctor|postdoctor], % 必选，学位类型
%   secret,                % 可选（默认：关闭），是否有密级
%   tocarialchapter,       % 可选（默认：关闭），章目录中使用黑体（这项表示同时打开下面两项）
%   tocarialchapterentry,  % 可选（默认：关闭），单独控制章标题在目录中使用黑体
%   tocarialchapterpage,   % 可选（默认：关闭），单独控制章页码在目录中使用黑体
%   pifootnote,            % 可选（默认：关闭），页脚编号采用 pifont 字体符号，建议打开

% 所有其它可能用到的包都统一放到这里了，可以根据自己的实际添加或者删除。
\usepackage{thuthesis}

% 下面的包让argmax之类的，_{}的东西显示在argmax正下方
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

% 定义所有的图片文件在 figures 子目录下
\graphicspath{{figures/}}

% 可以在这里修改配置文件中的定义。导言区可以使用中文。
\def\myname{王芮}

\begin{document}

%%% 封面部分å
%\frontmatter

%\input{data/cover}
% 如果使用授权说明扫描页，将可选参数中指定为扫描得到的 PDF 文件名，例如：
% \makecover[scan-auth.pdf]
%\makecover
%% 目录
% 设置目录层级，三级小标题全部显示
%\setcounter{secnumdepth}{5}
%\setcounter{tocdepth}{5}
%\tableofcontents

%% 符号对照表
%\input{data/denotation}


%%% 正文部分
\mainmatter
%\include{data/chap01}
%\include{data/chap02}
%\renewcommand\thesection{\arabic {section}} % use this command to disable chapter numbering

\section{Derivation}
我们的算法是分别对上下层策略进行更新，接下来我们证明，单独更新上层策略和单独更新下层策略都可以使得整体策略变优。

在接下来的推导过程中，我们用上标或下标$h$和$l$来表示强化学习中的一系列参数，是针对上层策略（high）还是下层策略（low）。

首先，我们证明，在下层策略不变的情况下，利用TRPO更新上层策略，可以保证整体策略的单调递增，即$V_h(s_0^h)$单调递增。这一步很简单，既然底层策略不变，那么可以认为底层策略是包含在环境中的，因此TRPO去优化上层策略，就相当于TRPO直接作用于一个单层的强化学习结构，对应的优化目标必然是越来越好的。注意到对于上层策略来说，它得到的$R$就是上下层策略作为一个整体得到的真实$R$。

接下来，我们推导，采用$ \frac{\gamma_h V_h(s_{t + k}^h) - V_h(s_{t}^h)}{k}$作为上层给下层的输入，可以使得整体策略变优。

我们的上下层优化算法采用的均是TRPO\cite{TRPO}。在TRPO当中，我们最终希望去优化的东西是$V_{\pi}(s_0)$。TRPO算法等价于在基于策略梯度定理给出的策略梯度优化中，增加了对步长的限制，从而确保策略更新的单调性。在策略梯度方法的论文\cite{policy_gradient_theorem}中，Sutton指出，对一个策略优化的目标函数$J$既可以采用$V_{\pi}(s_0)$的定义，也可以采用$R$的平均值的定义，它们可以分别推导得出策略梯度定理，而这个定理中的策略梯度，与TRPO试图优化的替代函数的梯度是相等的。因此，TRPO也可以理解成是在最大化$\mathbb{E}_{s, a \sim \pi}[R(s, a)]$。

如果把$R$的均值作为优化目标，我们有
\begin{align}
  \bigtriangledown J(\theta) = \bigtriangledown \mathbb{E}_{s\sim\pi}[\sum_{a} \pi(s, a) R(s, a)]
  &= \bigtriangledown \mathbb{E}_{s, a\sim\pi}[R(s, a)].
  \label{eq:policy_gradient}
\end{align}

我们定义了
\begin{align}
  R_l(s_{t + i}^l, a_{t + i}^l)|_{i = 0,1,...,k-1} = \frac{V_h(s_{t + k}^h) - V_h(s_{t}^h)}{k}.
  \label{eq:R_low}
\end{align}

前面我们提到，TRPO完成的任务是最大化$\mathbb{E}_{s, a \sim \pi}[R(s, a)]$，因此我们在利用TRPO针对底层策略进行优化的时候，相当于最大化$\mathbb{E}_{s, a \sim \pi_l, \pi_h}[R_l(s_l, a_l)]$。注意这里，我们把$\pi_h$看做是一个固定的概率分布函数，而不对它进行优化。$\pi_h$可以看做是环境的一部分，而它输出的隐式编码（latent code）则是我们观察（observation）的一部分。

根据\eqref{eq:R_low}中的结果，我们两边取期望，可以得到
\begin{align}
  \mathbb{E}_{s^l, a^l \sim \pi_l, \pi_h}[R_l(s^l, a^l)] = \frac{1}{k} \mathbb{E}_{s^h, a^h \sim \pi_l, \pi_h}[\gamma_h V_h(s_{t + k}^h) - V_h(s_{t}^h)].
  \label{eq:R_expectation}
\end{align}

回忆在强化学习中，我们关于优势函数$A(s, a)$的定义，为
\begin{align}
  A(S_t, A_t) = Q(S_t, A_t) - V(S_t) = R(S_t, A_t) + \gamma V(S_{t + 1}) - V(s).
  \label{eq:advantage}
\end{align}

由于在稀疏强化学习问题中有稀疏奖励条件：
\begin{align}
  R(S_t, A_t) = 0, \forall t \neq t_{end}.
  \label{eq:sparse_reward_condition}
\end{align}

因此，\eqref{eq:advantage}变为
\begin{align}
  A(S_t, A_t) = \gamma V(S_{t + 1}) - V(s).
  \label{eq:sparse_advantage}
\end{align}

由\eqref{eq:R_expectation}和\eqref{eq:sparse_advantage}可知，TRPO优化下层策略的结果，等效于优化了上层的优势函数，也就是
\begin{align}
  \mathbb{E}_{s^l, a^l \sim \pi_l, \pi_h}[R_l(s^l, a^l)] = \frac{1}{k} \mathbb{E}_{s^h, a^h \sim \pi_l, \pi_h}[A^h(s_t^h, a_t^h)].
  \label{eq:R_expectation_is_advantage}
\end{align}

接下来我们证明，通过优化这个期望值，就近似等效于优化了整体策略的表现。这里不能直接把TRPO的（13）搬过来用在上层policy上！TRPO是建立在环境动态不变的基础上的，但是我们的环境变了。必须follow TRPO论文的推导，看一下我们能不能推出一样的结果。下面做的就是这个事情。
%我们已经知道，TRPO最大化的替代函数的梯度，与策略梯度定理指出的梯度成正比。因而这个梯度，对应的需要最大化的函数，可以和TRPO（应用于上层策略）给出的替代函数相同。接下来我们推导在下层策略可能发生改变的情况下，上层策略的目标函数（也就是整体策略的目标函数）如何变化。

注意到，对于上层策略来说，底层策略相当于环境动力学（dynamics）。当底层策略发生了变化时，可以看做是环境动力学发生了变化。因此，即使上层策略不变，$S^h_t$的分布也可能发生改变。我们用$\mathcal{E}(\pi_l)$表示环境，并且环境与$\pi_l$有关。定义$\pi_h$带来的折扣奖励期望
\begin{equation}
\begin{aligned}
  \eta(\pi_h) = \mathbb{E}_{s_0^h, a_0^h, ...}\Bigg[\sum_{t = 0, k, 2k, ...} \gamma_h^{t/k} r_h(s_t^h, a_t^h)\Bigg] \text{ ,  where}   \\
  s_0^h \sim \rho_0^h(s_0^h), a_t^h \sim \pi_h(s_t^h), s_{t+1} \sim P(s_{t+1}^h|s_t^h, a_t^h, \mathcal{E}(\pi_l))
\end{aligned}
\end{equation}

我们用$\tilde{\eta}(\pi_h)$来表示$\pi_l$发生变化（变为$\tilde{\pi_l}$）而$\pi_h$没有发生变化以后的折扣奖励值期望。将TRPO根据\cite{TRPO_pre}得到的引理1（\textbf{Lemma 1}）稍加变形，我们可以得到类似的结果：（具体推导参见附录中证明1）
\begin{align}
  \tilde{\eta}(\pi_h) = \eta(\pi_h) + \mathbb{E}_{s_0^h, a_0^h, ... \sim \pi_h, \mathcal{E}(\tilde{\pi_l})}\Bigg[\sum_{t=0,k,2k,...} \gamma_h^{t/k} A_{\pi_h}(s_t^h, a_t^h)\Bigg].
  \label{eq:new_environment_advantage}
\end{align}
等式\eqref{eq:new_environment_advantage}右边第二项的含义就是，新的整体策略相对于旧的整体策略的优势。我们想要最大化的就是这个优势。

接下来，我们定义折扣访问频率$\rho$为
\begin{align}
  \rho_{\pi_h}(s^h) = \sum_{t = 0, k, 2k, ...}\gamma_h^{t/k}P(s_t^h = s|\mathcal{E}(\pi_l))
\end{align}
当底层策略发生改变而上层策略不变的时候，$\rho_{\pi_h}$相应地变为$\tilde{\rho}_{\pi_h}$。

表达式\eqref{eq:new_environment_advantage}中，等式右侧第二项可以变形为（参见TRPO论文中的等式（2）推导，写在附录中证明2）
\begin{align}
  \sum_{s^h}\tilde{\rho}_{\pi_h}(s^h)\sum_{a^h}\pi_h(a^h|s^h)A_{\pi_h}(s^h,a^h).
  \label{eq:accurate_objective}
\end{align}
进一步，由于$s^h \sim \tilde{\rho}_{\pi_h}(s^h)$难以采样，我们采用$s^h \sim \rho_{\pi_h}(s^h)$近似，从而定义出一个需要最大化的函数，为
\begin{align}
  \sum_{s^h}\rho_{\pi_h}(s^h)\sum_{a^h}\pi_h(a^h|s^h)A_{\pi_h}(s^h,a^h).
  \label{eq:surrogate_function_for_new_environment}
\end{align}
注意到在这里，我们采用$s^h \sim \rho_{\pi_h}(s^h)$来近似$s^h \sim \tilde{\rho}_{\pi_h}(s^h)$，与TRPO采用$s^h \sim \rho_{\pi_h}(s^h)$近似$s^h \sim \rho_{\tilde{\pi}_h}(s^h)$是不同的。是否可以证明\eqref{eq:surrogate_function_for_new_environment}与表达式\eqref{eq:accurate_objective}在$\pi_h$处的一阶泰勒展开相等？这里不会证！！如果这样，只要我们优化\eqref{eq:surrogate_function_for_new_environment}的步长足够小，就能够确保\eqref{eq:accurate_objective}也变大了。（此处还有一个问题：无法证明我们的step size可以保证单调性。因为底层的TRPO算法做的事情是让\eqref{eq:TRPO_high_level}单调变大，但是并没有限制它能变大多少。也就是说，并没有满足KL constraint。也就是说，底层的TRPO最大化的objective，不等效于上层的TRPO应该最大化的objective（少了一个constraint）。这就导致我们无法证明底层的TRPO能够让上层的结果也单调递增）。

这个替代函数的形式与TRPO中提出的替代函数形式类似，只不过在这里我们是利用旧环境的采样来替代新环境的采样，而TRPO是利用旧策略的采样来替代新策略的采样。我们可以用与TRPO相同的方法把式\eqref{eq:surrogate_function_for_new_environment}变为期望的形式，注意到与$\mathcal{E}(\pi_{\theta_{old}^l})$相关其实就是与$\pi_{\theta_{old}^l}$相关，因此我们简化表达式，略去$\mathcal{E}$，同时我们引入$\pi(a|s)$的参数$\theta$，得到
\begin{align}
  \mathbb{E}_{s^h \sim \rho_{\pi_h}, a^h \sim \pi_h }[\frac{\pi_\theta^h (a^h|s^h)}{\pi_{\theta_{old}}^h(a^h|s^h)}A_{\theta_{old}}^h(s^h, a^h)].
  \label{eq:TRPO_high_level}
\end{align}
在我们实际的算法中，状态$s$可以直接来自于采样。这是因为，当我们的训练样本足够大，并且选择随机起始时，可以认为$P(s_t = s)$对于不同的$t$是相同的值，因此，训练样本中的$s$也服从$\rho_{\pi_h}(s^h)$的分布。
\begin{align}
  \mathbb{E}_{s^h, a^h \sim \pi_{\theta_{old}^h},\pi_{\theta_{old}^l}} [\frac{\pi_\theta^h (a^h|s^h)}{\pi_{\theta_{old}}^h(a^h|s^h)}A_{\theta_{old}}^h(s^h, a^h)].
  \label{eq:TRPO_high_level}
\end{align}
注意到，由式\eqref{eq:R_expectation_is_advantage}，在$\pi_h$不变的条件下，我们针对底层策略$\pi^l$的更新，增大了$\mathbb{E}_{s^h, a^h \sim \pi_l, \pi_h}[A^h(s_t^h, a_t^h)]$，却没有影响$\frac{\pi_\theta^h (a^h|s^h)}{\pi_{\theta_{old}}^h(a^h|s^h)}$这一项（由于$\theta^h = \theta_{old}^h$，因此该项等于1）。这样，针对底层的更新确实使得表达式\eqref{eq:TRPO_high_level}变大了。但是这个表达式并不等于加了KL-constraint的替代函数，因此它的变大，并不能保证单调性。但在实际中，我们可以忽略掉这个限制。因此认为，底层策略的优化，就优化了整体策略度量$\eta(\pi_h)$的替代函数，从而优化了整体策略。

%%% 其它部分
\backmatter

%% 本科生要这几个索引，研究生不要。选择性留下。
% 插图索引
%\listoffigures
%% 表格索引
%\listoftables
%% 公式索引
%\listofequations


%% 参考文献
% 注意：至少需要引用一篇参考文献，否则下面两行可能引起编译错误。
% 如果不需要参考文献，请将下面两行删除或注释掉。
% 数字式引用
\bibliographystyle{thuthesis-numeric}
% 作者-年份式引用
% \bibliographystyle{thuthesis-author-year}
\bibliography{../../references/bibtex/all_references}


%% 致谢
%\include{data/ack}

%% 附录
\begin{appendix}

证明1源自TRPO论文的引理1，在表达式上有区别，但是思路相同。

\textbf{证明1：}
\begin{align}
  \tilde{\eta}(\pi_h) = \eta(\pi_h) + \mathbb{E}_{s_0^h, a_0^h, ... \sim \pi_h, \mathcal{E}(\tilde{\pi_l})}\Bigg[\sum_{t=0,k,2k,...} \gamma_h^{t/k} A_{\pi_h}(s_t^h, a_t^h)\Bigg].
  \label{eq:appendix1}
\end{align}

\textbf{证：}
\begin{align}
  &\mathbb{E}_{s_0^h, a_0^h, ... \sim \pi_h, \mathcal{E}(\tilde{\pi_l})}\Bigg[\sum_{t=0,k,2k,...} \gamma_h^{t/k} A_{\pi_h}(s_t^h, a_t^h)\Bigg]\\
  &= \mathbb{E}_{s_0^h, a_0^h, ... \sim \pi_h, \mathcal{E}(\tilde{\pi_l})}\Bigg[
      \sum_{t = 0, k, 2k, ...}\gamma_h^{t/k}(r_h(s_t^h)+\gamma_h V_{\pi_h}(s_{t+k}^h) - V_{\pi_h}(s_t^h))
      \Bigg]\\
  &= \mathbb{E}_{s_0^h, a_0^h, ... \sim \pi_h, \mathcal{E}(\tilde{\pi_l})}\Bigg[
      -V_{\pi_h}(s_0^h) + \sum_{t=0,k,2k,...} \gamma_h^{t/k} r_h(s_t^h)
      \Bigg]\\
  &= -\eta(\pi_h) + \tilde{\eta}(\pi_h)
\end{align}
存疑！这里的推导，前提条件是t一直走到无穷。对于我们这种episode会结束，并且sparse reward的情况，能否这么写？？

\textbf{证明2：}
\begin{align}
    \mathbb{E}_{s_0^h, a_0^h, ... \sim \pi_h, \mathcal{E}(\tilde{\pi_l})}\Bigg[\sum_{t=0,k,2k,...} \gamma_h^{t/k} A_{\pi_h}(s_t^h, a_t^h)\Bigg] = \sum_{s^h}\tilde{\rho}_{\pi_h}(s^h)\sum_{a^h}\pi_h(a^h|s^h)A_{\pi_h}(s^h,a^h).
\end{align}

\textbf{证：}
\begin{align}
  &\mathbb{E}_{s_0^h, a_0^h, ... \sim \pi_h, \mathcal{E}(\tilde{\pi_l})}\Bigg[\sum_{t=0,k,2k,...} \gamma_h^{t/k} A_{\pi_h}(s_t^h, a_t^h)\Bigg]\\
  &= \sum_{t = 0,k,2k,...} \sum_{s^h} P(s_t^h = s^h| \pi_h, \mathcal{E}(\pi_l)) \sum_{a^h} \pi_h(a^h|s^h) \gamma_h^{t/k} A_{\pi_h}(s^h, a^h)\\
  &= \sum_{s^h} \sum_{t = 0,k,2k,...} \gamma_h^{t/k} P(s_t^h = s^h|\pi_h, \mathcal{E}(\pi_l)) \sum_{a^h}\pi_h(a^h|s^h)A_{\pi_h}(s^h,a^h)\\
  &= \sum_{s^h}\tilde{\rho}_{\pi_h}(s^h)\sum_{a^h}\pi_h(a^h|s^h)A_{\pi_h}(s^h,a^h).
\end{align}

%\input{data/appendix01}
%\include{proposal/main_for_proposal}
%\includepdf[pages=-]{proposal/main_for_proposal.pdf}

\end{appendix}

%% 个人简历
%\include{data/resume}

%% 本科生进行格式审查是需要下面这个表格，答辩可能不需要。选择性留下。
% 综合论文训练记录表
%\includepdf[pages=-]{scan-record.pdf}

\end{document}
